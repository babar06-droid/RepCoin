{"dependencies":[{"name":"./engine/input_layer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":10,"column":0,"index":283},"end":{"line":10,"column":50,"index":333}}],"key":"qFF+yO5o1JEpzIoslHjbtoi9/Uo=","exportNames":["*"],"imports":1}},{"name":"./engine/topology","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":11,"column":0,"index":334},"end":{"line":11,"column":42,"index":376}}],"key":"mbsePG1gUDZHReR41fYpFHQ1dms=","exportNames":["*"],"imports":1}},{"name":"./exports","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":12,"column":0,"index":377},"end":{"line":12,"column":34,"index":411}}],"key":"YbTjfim1psbocC7cHZJcpca9/4c=","exportNames":["*"],"imports":1}},{"name":"./layers/advanced_activations","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":13,"column":0,"index":412},"end":{"line":13,"column":102,"index":514}}],"key":"AnCvyQkHadbR/fz60lhU/00N2d0=","exportNames":["*"],"imports":1}},{"name":"./layers/convolutional","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":14,"column":0,"index":515},"end":{"line":14,"column":141,"index":656}}],"key":"mwvllkWACp76NxsGuA6qWxJ5lL0=","exportNames":["*"],"imports":1}},{"name":"./layers/convolutional_depthwise","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":15,"column":0,"index":657},"end":{"line":15,"column":67,"index":724}}],"key":"dhO6BYLzM3QXWtobFm8ryIMbr+c=","exportNames":["*"],"imports":1}},{"name":"./layers/convolutional_recurrent","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":16,"column":0,"index":725},"end":{"line":16,"column":78,"index":803}}],"key":"mMZZGxwpomie1CDCNdnfn0vzAZo=","exportNames":["*"],"imports":1}},{"name":"./layers/core","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":17,"column":0,"index":804},"end":{"line":17,"column":127,"index":931}}],"key":"quJHoHj4jXwygZa/veRNZLf0k5s=","exportNames":["*"],"imports":1}},{"name":"./layers/embeddings","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":18,"column":0,"index":932},"end":{"line":18,"column":48,"index":980}}],"key":"LxfRg/zQdlNJ30uk7gDTVFRo8ps=","exportNames":["*"],"imports":1}},{"name":"./layers/merge","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":19,"column":0,"index":981},"end":{"line":19,"column":92,"index":1073}}],"key":"4F3hRiFroKltNiBWHB3AkOHv23s=","exportNames":["*"],"imports":1}},{"name":"./layers/noise","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":20,"column":0,"index":1074},"end":{"line":20,"column":78,"index":1152}}],"key":"JGr962qdPSfwMZ7cJvvGu4CGqUk=","exportNames":["*"],"imports":1}},{"name":"./layers/normalization","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":21,"column":0,"index":1153},"end":{"line":21,"column":80,"index":1233}}],"key":"pIX94ZbCd+RJtXEFnKGzI9NJThE=","exportNames":["*"],"imports":1}},{"name":"./layers/padding","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":22,"column":0,"index":1234},"end":{"line":22,"column":49,"index":1283}}],"key":"qfJ6kiTbv/4WxWM52CxODPJmPCM=","exportNames":["*"],"imports":1}},{"name":"./layers/pooling","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":23,"column":0,"index":1284},"end":{"line":23,"column":218,"index":1502}}],"key":"jrFu2FlQCz/mcWvgPWy+YrWeGd8=","exportNames":["*"],"imports":1}},{"name":"./layers/recurrent","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":24,"column":0,"index":1503},"end":{"line":24,"column":123,"index":1626}}],"key":"PUFaW8bMgNlls7FipuXNJ/U9M/g=","exportNames":["*"],"imports":1}},{"name":"./layers/wrappers","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":25,"column":0,"index":1627},"end":{"line":25,"column":67,"index":1694}}],"key":"gVnKhHu+pVzbWEEFbP3kpH4mQQ0=","exportNames":["*"],"imports":1}},{"name":"./layers/preprocessing/image_preprocessing","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":26,"column":0,"index":1695},"end":{"line":26,"column":71,"index":1766}}],"key":"zW123IatDDpGWEFp2OJc4UKpln8=","exportNames":["*"],"imports":1}},{"name":"./layers/preprocessing/center_crop","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":27,"column":0,"index":1767},"end":{"line":27,"column":64,"index":1831}}],"key":"nwl4sw4puVaqO3rKI1yxzLbE9uE=","exportNames":["*"],"imports":1}},{"name":"./layers/preprocessing/category_encoding","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":28,"column":0,"index":1832},"end":{"line":28,"column":76,"index":1908}}],"key":"TWyCHYjWTnouMQPZlQrc511qUao=","exportNames":["*"],"imports":1}},{"name":"./layers/preprocessing/image_resizing","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":29,"column":0,"index":1909},"end":{"line":29,"column":65,"index":1974}}],"key":"34RaO/F6ehLJYgIbn4FVcbcwjYQ=","exportNames":["*"],"imports":1}},{"name":"./layers/preprocessing/random_width","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":30,"column":0,"index":1975},"end":{"line":30,"column":66,"index":2041}}],"key":"VwRUpMN6ijCfu6pMJSk31EFZbvE=","exportNames":["*"],"imports":1}}],"output":[{"data":{"code":"__d(function (global, require, _$$_IMPORT_DEFAULT, _$$_IMPORT_ALL, module, exports, _dependencyMap) {\n  \"use strict\";\n\n  Object.defineProperty(exports, '__esModule', {\n    value: true\n  });\n  exports.inputLayer = inputLayer;\n  exports.elu = elu;\n  exports.reLU = reLU;\n  exports.leakyReLU = leakyReLU;\n  exports.prelu = prelu;\n  exports.softmax = softmax;\n  exports.thresholdedReLU = thresholdedReLU;\n  exports.conv1d = conv1d;\n  exports.conv2d = conv2d;\n  exports.conv2dTranspose = conv2dTranspose;\n  exports.conv3d = conv3d;\n  exports.conv3dTranspose = conv3dTranspose;\n  exports.separableConv2d = separableConv2d;\n  exports.cropping2D = cropping2D;\n  exports.upSampling2d = upSampling2d;\n  exports.depthwiseConv2d = depthwiseConv2d;\n  exports.activation = activation;\n  exports.dense = dense;\n  exports.dropout = dropout;\n  exports.spatialDropout1d = spatialDropout1d;\n  exports.flatten = flatten;\n  exports.repeatVector = repeatVector;\n  exports.reshape = reshape;\n  exports.permute = permute;\n  exports.embedding = embedding;\n  exports.add = add;\n  exports.average = average;\n  exports.concatenate = concatenate;\n  exports.maximum = maximum;\n  exports.minimum = minimum;\n  exports.multiply = multiply;\n  exports.dot = dot;\n  exports.batchNormalization = batchNormalization;\n  exports.layerNormalization = layerNormalization;\n  exports.zeroPadding2d = zeroPadding2d;\n  exports.averagePooling1d = averagePooling1d;\n  exports.avgPool1d = avgPool1d;\n  exports.avgPooling1d = avgPooling1d;\n  exports.averagePooling2d = averagePooling2d;\n  exports.avgPool2d = avgPool2d;\n  exports.avgPooling2d = avgPooling2d;\n  exports.averagePooling3d = averagePooling3d;\n  exports.avgPool3d = avgPool3d;\n  exports.avgPooling3d = avgPooling3d;\n  exports.globalAveragePooling1d = globalAveragePooling1d;\n  exports.globalAveragePooling2d = globalAveragePooling2d;\n  exports.globalMaxPooling1d = globalMaxPooling1d;\n  exports.globalMaxPooling2d = globalMaxPooling2d;\n  exports.maxPooling1d = maxPooling1d;\n  exports.maxPooling2d = maxPooling2d;\n  exports.maxPooling3d = maxPooling3d;\n  exports.gru = gru;\n  exports.gruCell = gruCell;\n  exports.lstm = lstm;\n  exports.lstmCell = lstmCell;\n  exports.simpleRNN = simpleRNN;\n  exports.simpleRNNCell = simpleRNNCell;\n  exports.convLstm2d = convLstm2d;\n  exports.convLstm2dCell = convLstm2dCell;\n  exports.rnn = rnn;\n  exports.stackedRNNCells = stackedRNNCells;\n  exports.bidirectional = bidirectional;\n  exports.timeDistributed = timeDistributed;\n  Object.defineProperty(exports, \"globalMaxPool1d\", {\n    enumerable: true,\n    get: function () {\n      return globalMaxPool1d;\n    }\n  });\n  Object.defineProperty(exports, \"globalMaxPool2d\", {\n    enumerable: true,\n    get: function () {\n      return globalMaxPool2d;\n    }\n  });\n  Object.defineProperty(exports, \"maxPool1d\", {\n    enumerable: true,\n    get: function () {\n      return maxPool1d;\n    }\n  });\n  Object.defineProperty(exports, \"maxPool2d\", {\n    enumerable: true,\n    get: function () {\n      return maxPool2d;\n    }\n  });\n  Object.defineProperty(exports, \"Layer\", {\n    enumerable: true,\n    get: function () {\n      return _engineTopology.Layer;\n    }\n  });\n  Object.defineProperty(exports, \"RNN\", {\n    enumerable: true,\n    get: function () {\n      return _layersRecurrent.RNN;\n    }\n  });\n  Object.defineProperty(exports, \"RNNCell\", {\n    enumerable: true,\n    get: function () {\n      return _layersRecurrent.RNNCell;\n    }\n  });\n  Object.defineProperty(exports, \"input\", {\n    enumerable: true,\n    get: function () {\n      return _exports2.input;\n    }\n  });\n  exports.gaussianNoise = gaussianNoise;\n  exports.gaussianDropout = gaussianDropout;\n  exports.alphaDropout = alphaDropout;\n  exports.masking = masking;\n  exports.rescaling = rescaling;\n  exports.centerCrop = centerCrop;\n  exports.resizing = resizing;\n  exports.categoryEncoding = categoryEncoding;\n  exports.randomWidth = randomWidth;\n  var _engineInput_layer = require(_dependencyMap[0], \"./engine/input_layer\");\n  var _engineTopology = require(_dependencyMap[1], \"./engine/topology\");\n  var _exports2 = require(_dependencyMap[2], \"./exports\");\n  var _layersAdvanced_activations = require(_dependencyMap[3], \"./layers/advanced_activations\");\n  var _layersConvolutional = require(_dependencyMap[4], \"./layers/convolutional\");\n  var _layersConvolutional_depthwise = require(_dependencyMap[5], \"./layers/convolutional_depthwise\");\n  var _layersConvolutional_recurrent = require(_dependencyMap[6], \"./layers/convolutional_recurrent\");\n  var _layersCore = require(_dependencyMap[7], \"./layers/core\");\n  var _layersEmbeddings = require(_dependencyMap[8], \"./layers/embeddings\");\n  var _layersMerge = require(_dependencyMap[9], \"./layers/merge\");\n  var _layersNoise = require(_dependencyMap[10], \"./layers/noise\");\n  var _layersNormalization = require(_dependencyMap[11], \"./layers/normalization\");\n  var _layersPadding = require(_dependencyMap[12], \"./layers/padding\");\n  var _layersPooling = require(_dependencyMap[13], \"./layers/pooling\");\n  var _layersRecurrent = require(_dependencyMap[14], \"./layers/recurrent\");\n  var _layersWrappers = require(_dependencyMap[15], \"./layers/wrappers\");\n  var _layersPreprocessingImage_preprocessing = require(_dependencyMap[16], \"./layers/preprocessing/image_preprocessing\");\n  var _layersPreprocessingCenter_crop = require(_dependencyMap[17], \"./layers/preprocessing/center_crop\");\n  var _layersPreprocessingCategory_encoding = require(_dependencyMap[18], \"./layers/preprocessing/category_encoding\");\n  var _layersPreprocessingImage_resizing = require(_dependencyMap[19], \"./layers/preprocessing/image_resizing\");\n  var _layersPreprocessingRandom_width = require(_dependencyMap[20], \"./layers/preprocessing/random_width\");\n  /**\n   * @license\n   * Copyright 2018 Google LLC\n   *\n   * Use of this source code is governed by an MIT-style\n   * license that can be found in the LICENSE file or at\n   * https://opensource.org/licenses/MIT.\n   * =============================================================================\n   */\n\n  // TODO(cais): Add doc string to all the public static functions in this\n  //   class; include exectuable JavaScript code snippets where applicable\n  //   (b/74074458).\n  // Input Layer.\n  /**\n   * An input layer is an entry point into a `tf.LayersModel`.\n   *\n   * `InputLayer` is generated automatically for `tf.Sequential` models by\n   * specifying the `inputshape` or `batchInputShape` for the first layer.  It\n   * should not be specified explicitly. However, it can be useful sometimes,\n   * e.g., when constructing a sequential model from a subset of another\n   * sequential model's layers. Like the code snippet below shows.\n   *\n   * ```js\n   * // Define a model which simply adds two inputs.\n   * const model1 = tf.sequential();\n   * model1.add(tf.layers.dense({inputShape: [4], units: 3, activation: 'relu'}));\n   * model1.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));\n   * model1.summary();\n   * model1.predict(tf.zeros([1, 4])).print();\n   *\n   * // Construct another model, reusing the second layer of `model1` while\n   * // not using the first layer of `model1`. Note that you cannot add the second\n   * // layer of `model` directly as the first layer of the new sequential model,\n   * // because doing so will lead to an error related to the fact that the layer\n   * // is not an input layer. Instead, you need to create an `inputLayer` and add\n   * // it to the new sequential model before adding the reused layer.\n   * const model2 = tf.sequential();\n   * // Use an inputShape that matches the input shape of `model1`'s second\n   * // layer.\n   * model2.add(tf.layers.inputLayer({inputShape: [3]}));\n   * model2.add(model1.layers[1]);\n   * model2.summary();\n   * model2.predict(tf.zeros([1, 3])).print();\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Inputs', namespace: 'layers'}\n   */\n  function inputLayer(args) {\n    return new _engineInput_layer.InputLayer(args);\n  }\n  // Advanced Activation Layers.\n  /**\n   * Exponential Linear Unit (ELU).\n   *\n   * It follows:\n   * `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n   * `f(x) = x for x >= 0`.\n   *\n   * Input shape:\n   *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n   *   first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as the input.\n   *\n   * References:\n   *   - [Fast and Accurate Deep Network Learning by Exponential Linear Units\n   * (ELUs)](https://arxiv.org/abs/1511.07289v1)\n   *\n   * @doc {\n   *   heading: 'Layers',\n   *   subheading: 'Advanced Activation',\n   *   namespace: 'layers'\n   * }\n   */\n  function elu(args) {\n    return new _layersAdvanced_activations.ELU(args);\n  }\n  /**\n   * Rectified Linear Unit activation function.\n   *\n   * Input shape:\n   *   Arbitrary. Use the config field `inputShape` (Array of integers, does\n   *   not include the sample axis) when using this layer as the first layer\n   *   in a model.\n   *\n   * Output shape:\n   *   Same shape as the input.\n   *\n   * @doc {\n   *   heading: 'Layers',\n   *   subheading: 'Advanced Activation',\n   *   namespace: 'layers'\n   * }\n   */\n  function reLU(args) {\n    return new _layersAdvanced_activations.ReLU(args);\n  }\n  /**\n   * Leaky version of a rectified linear unit.\n   *\n   * It allows a small gradient when the unit is not active:\n   * `f(x) = alpha * x for x < 0.`\n   * `f(x) = x for x >= 0.`\n   *\n   * Input shape:\n   *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n   *   first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as the input.\n   *\n   * @doc {\n   *   heading: 'Layers',\n   *   subheading: 'Advanced Activation',\n   *   namespace: 'layers'\n   * }\n   */\n  function leakyReLU(args) {\n    return new _layersAdvanced_activations.LeakyReLU(args);\n  }\n  /**\n   * Parameterized version of a leaky rectified linear unit.\n   *\n   * It follows\n   * `f(x) = alpha * x for x < 0.`\n   * `f(x) = x for x >= 0.`\n   * wherein `alpha` is a trainable weight.\n   *\n   * Input shape:\n   *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n   *   first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as the input.\n   *\n   * @doc {\n   *   heading: 'Layers',\n   *   subheading: 'Advanced Activation',\n   *   namespace: 'layers'\n   * }\n   */\n  function prelu(args) {\n    return new _layersAdvanced_activations.PReLU(args);\n  }\n  /**\n   * Softmax activation layer.\n   *\n   * Input shape:\n   *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n   *   first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as the input.\n   *\n   * @doc {\n   *   heading: 'Layers',\n   *   subheading: 'Advanced Activation',\n   *   namespace: 'layers'\n   * }\n   */\n  function softmax(args) {\n    return new _layersAdvanced_activations.Softmax(args);\n  }\n  /**\n   * Thresholded Rectified Linear Unit.\n   *\n   * It follows:\n   * `f(x) = x for x > theta`,\n   * `f(x) = 0 otherwise`.\n   *\n   * Input shape:\n   *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n   *   first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as the input.\n   *\n   * References:\n   *   - [Zero-Bias Autoencoders and the Benefits of Co-Adapting\n   * Features](http://arxiv.org/abs/1402.3337)\n   *\n   * @doc {\n   *   heading: 'Layers',\n   *   subheading: 'Advanced Activation',\n   *   namespace: 'layers'\n   * }\n   */\n  function thresholdedReLU(args) {\n    return new _layersAdvanced_activations.ThresholdedReLU(args);\n  }\n  // Convolutional Layers.\n  /**\n   * 1D convolution layer (e.g., temporal convolution).\n   *\n   * This layer creates a convolution kernel that is convolved\n   * with the layer input over a single spatial (or temporal) dimension\n   * to produce a tensor of outputs.\n   *\n   * If `use_bias` is True, a bias vector is created and added to the outputs.\n   *\n   * If `activation` is not `null`, it is applied to the outputs as well.\n   *\n   * When using this layer as the first layer in a model, provide an\n   * `inputShape` argument `Array` or `null`.\n   *\n   * For example, `inputShape` would be:\n   * - `[10, 128]` for sequences of 10 vectors of 128-dimensional vectors\n   * - `[null, 128]` for variable-length sequences of 128-dimensional vectors.\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional',  namespace: 'layers'}\n   */\n  function conv1d(args) {\n    return new _layersConvolutional.Conv1D(args);\n  }\n  /**\n   * 2D convolution layer (e.g. spatial convolution over images).\n   *\n   * This layer creates a convolution kernel that is convolved\n   * with the layer input to produce a tensor of outputs.\n   *\n   * If `useBias` is True, a bias vector is created and added to the outputs.\n   *\n   * If `activation` is not `null`, it is applied to the outputs as well.\n   *\n   * When using this layer as the first layer in a model,\n   * provide the keyword argument `inputShape`\n   * (Array of integers, does not include the sample axis),\n   * e.g. `inputShape=[128, 128, 3]` for 128x128 RGB pictures\n   * in `dataFormat='channelsLast'`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n   */\n  function conv2d(args) {\n    return new _layersConvolutional.Conv2D(args);\n  }\n  /**\n   * Transposed convolutional layer (sometimes called Deconvolution).\n   *\n   * The need for transposed convolutions generally arises\n   * from the desire to use a transformation going in the opposite direction of\n   * a normal convolution, i.e., from something that has the shape of the output\n   * of some convolution to something that has the shape of its input while\n   * maintaining a connectivity pattern that is compatible with said\n   * convolution.\n   *\n   * When using this layer as the first layer in a model, provide the\n   * configuration `inputShape` (`Array` of integers, does not include the\n   * sample axis), e.g., `inputShape: [128, 128, 3]` for 128x128 RGB pictures in\n   * `dataFormat: 'channelsLast'`.\n   *\n   * Input shape:\n   *   4D tensor with shape:\n   *   `[batch, channels, rows, cols]` if `dataFormat` is `'channelsFirst'`.\n   *   or 4D tensor with shape\n   *   `[batch, rows, cols, channels]` if `dataFormat` is `'channelsLast'`.\n   *\n   * Output shape:\n   *   4D tensor with shape:\n   *   `[batch, filters, newRows, newCols]` if `dataFormat` is\n   * `'channelsFirst'`. or 4D tensor with shape:\n   *   `[batch, newRows, newCols, filters]` if `dataFormat` is `'channelsLast'`.\n   *\n   * References:\n   *   - [A guide to convolution arithmetic for deep\n   * learning](https://arxiv.org/abs/1603.07285v1)\n   *   - [Deconvolutional\n   * Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n   */\n  function conv2dTranspose(args) {\n    return new _layersConvolutional.Conv2DTranspose(args);\n  }\n  /**\n   * 3D convolution layer (e.g. spatial convolution over volumes).\n   *\n   * This layer creates a convolution kernel that is convolved\n   * with the layer input to produce a tensor of outputs.\n   *\n   * If `useBias` is True, a bias vector is created and added to the outputs.\n   *\n   * If `activation` is not `null`, it is applied to the outputs as well.\n   *\n   * When using this layer as the first layer in a model,\n   * provide the keyword argument `inputShape`\n   * (Array of integers, does not include the sample axis),\n   * e.g. `inputShape=[128, 128, 128, 1]` for 128x128x128 grayscale volumes\n   * in `dataFormat='channelsLast'`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n   */\n  function conv3d(args) {\n    return new _layersConvolutional.Conv3D(args);\n  }\n  function conv3dTranspose(args) {\n    return new _layersConvolutional.Conv3DTranspose(args);\n  }\n  /**\n   * Depthwise separable 2D convolution.\n   *\n   * Separable convolution consists of first performing\n   * a depthwise spatial convolution\n   * (which acts on each input channel separately)\n   * followed by a pointwise convolution which mixes together the resulting\n   * output channels. The `depthMultiplier` argument controls how many\n   * output channels are generated per input channel in the depthwise step.\n   *\n   * Intuitively, separable convolutions can be understood as\n   * a way to factorize a convolution kernel into two smaller kernels,\n   * or as an extreme version of an Inception block.\n   *\n   * Input shape:\n   *   4D tensor with shape:\n   *     `[batch, channels, rows, cols]` if data_format='channelsFirst'\n   *   or 4D tensor with shape:\n   *     `[batch, rows, cols, channels]` if data_format='channelsLast'.\n   *\n   * Output shape:\n   *   4D tensor with shape:\n   *     `[batch, filters, newRows, newCols]` if data_format='channelsFirst'\n   *   or 4D tensor with shape:\n   *     `[batch, newRows, newCols, filters]` if data_format='channelsLast'.\n   *     `rows` and `cols` values might have changed due to padding.\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n   */\n  function separableConv2d(args) {\n    return new _layersConvolutional.SeparableConv2D(args);\n  }\n  /**\n   * Cropping layer for 2D input (e.g., image).\n   *\n   * This layer can crop an input\n   * at the top, bottom, left and right side of an image tensor.\n   *\n   * Input shape:\n   *   4D tensor with shape:\n   *   - If `dataFormat` is `\"channelsLast\"`:\n   *     `[batch, rows, cols, channels]`\n   *   - If `data_format` is `\"channels_first\"`:\n   *     `[batch, channels, rows, cols]`.\n   *\n   * Output shape:\n   *   4D with shape:\n   *   - If `dataFormat` is `\"channelsLast\"`:\n   *     `[batch, croppedRows, croppedCols, channels]`\n   *    - If `dataFormat` is `\"channelsFirst\"`:\n   *     `[batch, channels, croppedRows, croppedCols]`.\n   *\n   * Examples\n   * ```js\n   *\n   * const model = tf.sequential();\n   * model.add(tf.layers.cropping2D({cropping:[[2, 2], [2, 2]],\n   *                                inputShape: [128, 128, 3]}));\n   * //now output shape is [batch, 124, 124, 3]\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n   */\n  function cropping2D(args) {\n    return new _layersConvolutional.Cropping2D(args);\n  }\n  /**\n   * Upsampling layer for 2D inputs.\n   *\n   * Repeats the rows and columns of the data\n   * by size[0] and size[1] respectively.\n   *\n   *\n   * Input shape:\n   *    4D tensor with shape:\n   *     - If `dataFormat` is `\"channelsLast\"`:\n   *         `[batch, rows, cols, channels]`\n   *     - If `dataFormat` is `\"channelsFirst\"`:\n   *        `[batch, channels, rows, cols]`\n   *\n   * Output shape:\n   *     4D tensor with shape:\n   *     - If `dataFormat` is `\"channelsLast\"`:\n   *        `[batch, upsampledRows, upsampledCols, channels]`\n   *     - If `dataFormat` is `\"channelsFirst\"`:\n   *         `[batch, channels, upsampledRows, upsampledCols]`\n   *\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n   */\n  function upSampling2d(args) {\n    return new _layersConvolutional.UpSampling2D(args);\n  }\n  // Convolutional(depthwise) Layers.\n  /**\n   * Depthwise separable 2D convolution.\n   *\n   * Depthwise Separable convolutions consists in performing just the first step\n   * in a depthwise spatial convolution (which acts on each input channel\n   * separately). The `depthMultiplier` argument controls how many output channels\n   * are generated per input channel in the depthwise step.\n   *\n   * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n   */\n  function depthwiseConv2d(args) {\n    return new _layersConvolutional_depthwise.DepthwiseConv2D(args);\n  }\n  // Basic Layers.\n  /**\n   * Applies an activation function to an output.\n   *\n   * This layer applies element-wise activation function.  Other layers, notably\n   * `dense` can also apply activation functions.  Use this isolated activation\n   * function to extract the values before and after the\n   * activation. For instance:\n   *\n   * ```js\n   * const input = tf.input({shape: [5]});\n   * const denseLayer = tf.layers.dense({units: 1});\n   * const activationLayer = tf.layers.activation({activation: 'relu6'});\n   *\n   * // Obtain the output symbolic tensors by applying the layers in order.\n   * const denseOutput = denseLayer.apply(input);\n   * const activationOutput = activationLayer.apply(denseOutput);\n   *\n   * // Create the model based on the inputs.\n   * const model = tf.model({\n   *     inputs: input,\n   *     outputs: [denseOutput, activationOutput]\n   * });\n   *\n   * // Collect both outputs and print separately.\n   * const [denseOut, activationOut] = model.predict(tf.randomNormal([6, 5]));\n   * denseOut.print();\n   * activationOut.print();\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function activation(args) {\n    return new _layersCore.Activation(args);\n  }\n  /**\n   * Creates a dense (fully connected) layer.\n   *\n   * This layer implements the operation:\n   *   `output = activation(dot(input, kernel) + bias)`\n   *\n   * `activation` is the element-wise activation function\n   *   passed as the `activation` argument.\n   *\n   * `kernel` is a weights matrix created by the layer.\n   *\n   * `bias` is a bias vector created by the layer (only applicable if `useBias`\n   * is `true`).\n   *\n   * **Input shape:**\n   *\n   *   nD `tf.Tensor` with shape: `(batchSize, ..., inputDim)`.\n   *\n   *   The most common situation would be\n   *   a 2D input with shape `(batchSize, inputDim)`.\n   *\n   * **Output shape:**\n   *\n   *   nD tensor with shape: `(batchSize, ..., units)`.\n   *\n   *   For instance, for a 2D input with shape `(batchSize, inputDim)`,\n   *   the output would have shape `(batchSize, units)`.\n   *\n   * Note: if the input to the layer has a rank greater than 2, then it is\n   * flattened prior to the initial dot product with the kernel.\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function dense(args) {\n    return new _layersCore.Dense(args);\n  }\n  /**\n   * Applies\n   * [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) to\n   * the input.\n   *\n   * Dropout consists in randomly setting a fraction `rate` of input units to 0 at\n   * each update during training time, which helps prevent overfitting.\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function dropout(args) {\n    return new _layersCore.Dropout(args);\n  }\n  /**\n   * Spatial 1D version of Dropout.\n   *\n   * This Layer type performs the same function as the Dropout layer, but it drops\n   * entire 1D feature maps instead of individual elements. For example, if an\n   * input example consists of 3 timesteps and the feature map for each timestep\n   * has a size of 4, a `spatialDropout1d` layer may zero out the feature maps\n   * of the 1st timesteps and 2nd timesteps completely while sparing all feature\n   * elements of the 3rd timestep.\n   *\n   * If adjacent frames (timesteps) are strongly correlated (as is normally the\n   * case in early convolution layers), regular dropout will not regularize the\n   * activation and will otherwise just result in merely an effective learning\n   * rate decrease. In this case, `spatialDropout1d` will help promote\n   * independence among feature maps and should be used instead.\n   *\n   * **Arguments:**\n   *   rate: A floating-point number >=0 and <=1. Fraction of the input elements\n   *     to drop.\n   *\n   * **Input shape:**\n   *   3D tensor with shape `(samples, timesteps, channels)`.\n   *\n   * **Output shape:**\n   *   Same as the input shape.\n   *\n   * References:\n   *   - [Efficient Object Localization Using Convolutional\n   *      Networks](https://arxiv.org/abs/1411.4280)\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function spatialDropout1d(args) {\n    return new _layersCore.SpatialDropout1D(args);\n  }\n  /**\n   * Flattens the input. Does not affect the batch size.\n   *\n   * A `Flatten` layer flattens each batch in its inputs to 1D (making the output\n   * 2D).\n   *\n   * For example:\n   *\n   * ```js\n   * const input = tf.input({shape: [4, 3]});\n   * const flattenLayer = tf.layers.flatten();\n   * // Inspect the inferred output shape of the flatten layer, which\n   * // equals `[null, 12]`. The 2nd dimension is 4 * 3, i.e., the result of the\n   * // flattening. (The 1st dimension is the undermined batch size.)\n   * console.log(JSON.stringify(flattenLayer.apply(input).shape));\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function flatten(args) {\n    return new _layersCore.Flatten(args);\n  }\n  /**\n   * Repeats the input n times in a new dimension.\n   *\n   * ```js\n   *  const model = tf.sequential();\n   *  model.add(tf.layers.repeatVector({n: 4, inputShape: [2]}));\n   *  const x = tf.tensor2d([[10, 20]]);\n   *  // Use the model to do inference on a data point the model hasn't seen\n   *  model.predict(x).print();\n   *  // output shape is now [batch, 2, 4]\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function repeatVector(args) {\n    return new _layersCore.RepeatVector(args);\n  }\n  /**\n   * Reshapes an input to a certain shape.\n   *\n   * ```js\n   * const input = tf.input({shape: [4, 3]});\n   * const reshapeLayer = tf.layers.reshape({targetShape: [2, 6]});\n   * // Inspect the inferred output shape of the Reshape layer, which\n   * // equals `[null, 2, 6]`. (The 1st dimension is the undermined batch size.)\n   * console.log(JSON.stringify(reshapeLayer.apply(input).shape));\n   * ```\n   *\n   * Input shape:\n   *   Arbitrary, although all dimensions in the input shape must be fixed.\n   *   Use the configuration `inputShape` when using this layer as the\n   *   first layer in a model.\n   *\n   *\n   * Output shape:\n   *   [batchSize, targetShape[0], targetShape[1], ...,\n   *    targetShape[targetShape.length - 1]].\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function reshape(args) {\n    return new _layersCore.Reshape(args);\n  }\n  /**\n   * Permutes the dimensions of the input according to a given pattern.\n   *\n   * Useful for, e.g., connecting RNNs and convnets together.\n   *\n   * Example:\n   *\n   * ```js\n   * const model = tf.sequential();\n   * model.add(tf.layers.permute({\n   *   dims: [2, 1],\n   *   inputShape: [10, 64]\n   * }));\n   * console.log(model.outputShape);\n   * // Now model's output shape is [null, 64, 10], where null is the\n   * // unpermuted sample (batch) dimension.\n   * ```\n   *\n   * Input shape:\n   *   Arbitrary. Use the configuration field `inputShape` when using this\n   *   layer as the first layer in a model.\n   *\n   * Output shape:\n   *   Same rank as the input shape, but with the dimensions re-ordered (i.e.,\n   *   permuted) according to the `dims` configuration of this layer.\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function permute(args) {\n    return new _layersCore.Permute(args);\n  }\n  /**\n   * Maps positive integers (indices) into dense vectors of fixed size.\n   * E.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n   *\n   * **Input shape:** 2D tensor with shape: `[batchSize, sequenceLength]`.\n   *\n   * **Output shape:** 3D tensor with shape: `[batchSize, sequenceLength,\n   * outputDim]`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n   */\n  function embedding(args) {\n    return new _layersEmbeddings.Embedding(args);\n  }\n  // Merge Layers.\n  /**\n   * Layer that performs element-wise addition on an `Array` of inputs.\n   *\n   * It takes as input a list of tensors, all of the same shape, and returns a\n   * single tensor (also of the same shape). The inputs are specified as an\n   * `Array` when the `apply` method of the `Add` layer instance is called. For\n   * example:\n   *\n   * ```js\n   * const input1 = tf.input({shape: [2, 2]});\n   * const input2 = tf.input({shape: [2, 2]});\n   * const addLayer = tf.layers.add();\n   * const sum = addLayer.apply([input1, input2]);\n   * console.log(JSON.stringify(sum.shape));\n   * // You get [null, 2, 2], with the first dimension as the undetermined batch\n   * // dimension.\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n   */\n  function add(args) {\n    return new _layersMerge.Add(args);\n  }\n  /**\n   * Layer that performs element-wise averaging on an `Array` of inputs.\n   *\n   * It takes as input a list of tensors, all of the same shape, and returns a\n   * single tensor (also of the same shape). For example:\n   *\n   * ```js\n   * const input1 = tf.input({shape: [2, 2]});\n   * const input2 = tf.input({shape: [2, 2]});\n   * const averageLayer = tf.layers.average();\n   * const average = averageLayer.apply([input1, input2]);\n   * console.log(JSON.stringify(average.shape));\n   * // You get [null, 2, 2], with the first dimension as the undetermined batch\n   * // dimension.\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n   */\n  function average(args) {\n    return new _layersMerge.Average(args);\n  }\n  /**\n   * Layer that concatenates an `Array` of inputs.\n   *\n   * It takes a list of tensors, all of the same shape except for the\n   * concatenation axis, and returns a single tensor, the concatenation\n   * of all inputs. For example:\n   *\n   * ```js\n   * const input1 = tf.input({shape: [2, 2]});\n   * const input2 = tf.input({shape: [2, 3]});\n   * const concatLayer = tf.layers.concatenate();\n   * const output = concatLayer.apply([input1, input2]);\n   * console.log(JSON.stringify(output.shape));\n   * // You get [null, 2, 5], with the first dimension as the undetermined batch\n   * // dimension. The last dimension (5) is the result of concatenating the\n   * // last dimensions of the inputs (2 and 3).\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n   */\n  function concatenate(args) {\n    return new _layersMerge.Concatenate(args);\n  }\n  /**\n   * Layer that computes the element-wise maximum of an `Array` of inputs.\n   *\n   * It takes as input a list of tensors, all of the same shape, and returns a\n   * single tensor (also of the same shape). For example:\n   *\n   * ```js\n   * const input1 = tf.input({shape: [2, 2]});\n   * const input2 = tf.input({shape: [2, 2]});\n   * const maxLayer = tf.layers.maximum();\n   * const max = maxLayer.apply([input1, input2]);\n   * console.log(JSON.stringify(max.shape));\n   * // You get [null, 2, 2], with the first dimension as the undetermined batch\n   * // dimension.\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n   */\n  function maximum(args) {\n    return new _layersMerge.Maximum(args);\n  }\n  /**\n   * Layer that computes the element-wise minimum of an `Array` of inputs.\n   *\n   * It takes as input a list of tensors, all of the same shape, and returns a\n   * single tensor (also of the same shape). For example:\n   *\n   * ```js\n   * const input1 = tf.input({shape: [2, 2]});\n   * const input2 = tf.input({shape: [2, 2]});\n   * const minLayer = tf.layers.minimum();\n   * const min = minLayer.apply([input1, input2]);\n   * console.log(JSON.stringify(min.shape));\n   * // You get [null, 2, 2], with the first dimension as the undetermined batch\n   * // dimension.\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n   */\n  function minimum(args) {\n    return new _layersMerge.Minimum(args);\n  }\n  /**\n   * Layer that multiplies (element-wise) an `Array` of inputs.\n   *\n   * It takes as input an Array of tensors, all of the same\n   * shape, and returns a single tensor (also of the same shape).\n   * For example:\n   *\n   * ```js\n   * const input1 = tf.input({shape: [2, 2]});\n   * const input2 = tf.input({shape: [2, 2]});\n   * const input3 = tf.input({shape: [2, 2]});\n   * const multiplyLayer = tf.layers.multiply();\n   * const product = multiplyLayer.apply([input1, input2, input3]);\n   * console.log(product.shape);\n   * // You get [null, 2, 2], with the first dimension as the undetermined batch\n   * // dimension.\n   *\n   * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n   */\n  function multiply(args) {\n    return new _layersMerge.Multiply(args);\n  }\n  /**\n   * Layer that computes a dot product between samples in two tensors.\n   *\n   * E.g., if applied to a list of two tensors `a` and `b` both of shape\n   * `[batchSize, n]`, the output will be a tensor of shape `[batchSize, 1]`,\n   * where each entry at index `[i, 0]` will be the dot product between\n   * `a[i, :]` and `b[i, :]`.\n   *\n   * Example:\n   *\n   * ```js\n   * const dotLayer = tf.layers.dot({axes: -1});\n   * const x1 = tf.tensor2d([[10, 20], [30, 40]]);\n   * const x2 = tf.tensor2d([[-1, -2], [-3, -4]]);\n   *\n   * // Invoke the layer's apply() method in eager (imperative) mode.\n   * const y = dotLayer.apply([x1, x2]);\n   * y.print();\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n   */\n  function dot(args) {\n    return new _layersMerge.Dot(args);\n  }\n  // Normalization Layers.\n  /**\n   * Batch normalization layer (Ioffe and Szegedy, 2014).\n   *\n   * Normalize the activations of the previous layer at each batch,\n   * i.e. applies a transformation that maintains the mean activation\n   * close to 0 and the activation standard deviation close to 1.\n   *\n   * Input shape:\n   *   Arbitrary. Use the keyword argument `inputShape` (Array of integers, does\n   *   not include the sample axis) when calling the constructor of this class,\n   *   if this layer is used as a first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as input.\n   *\n   * References:\n   *   - [Batch Normalization: Accelerating Deep Network Training by Reducing\n   * Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n   *\n   * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n   */\n  function batchNormalization(args) {\n    return new _layersNormalization.BatchNormalization(args);\n  }\n  /**\n   * Layer-normalization layer (Ba et al., 2016).\n   *\n   * Normalizes the activations of the previous layer for each given example in a\n   * batch independently, instead of across a batch like in `batchNormalization`.\n   * In other words, this layer applies a transformation that maintains the mean\n   * activation within each example close to 0 and activation variance close to 1.\n   *\n   * Input shape:\n   *   Arbitrary. Use the argument `inputShape` when using this layer as the first\n   *   layer in a model.\n   *\n   * Output shape:\n   *   Same as input.\n   *\n   * References:\n   *   - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n   *\n   * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n   */\n  function layerNormalization(args) {\n    return new _layersNormalization.LayerNormalization(args);\n  }\n  // Padding Layers.\n  /**\n   * Zero-padding layer for 2D input (e.g., image).\n   *\n   * This layer can add rows and columns of zeros\n   * at the top, bottom, left and right side of an image tensor.\n   *\n   * Input shape:\n   *   4D tensor with shape:\n   *   - If `dataFormat` is `\"channelsLast\"`:\n   *     `[batch, rows, cols, channels]`\n   *   - If `data_format` is `\"channels_first\"`:\n   *     `[batch, channels, rows, cols]`.\n   *\n   * Output shape:\n   *   4D with shape:\n   *   - If `dataFormat` is `\"channelsLast\"`:\n   *     `[batch, paddedRows, paddedCols, channels]`\n   *    - If `dataFormat` is `\"channelsFirst\"`:\n   *     `[batch, channels, paddedRows, paddedCols]`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Padding', namespace: 'layers'}\n   */\n  function zeroPadding2d(args) {\n    return new _layersPadding.ZeroPadding2D(args);\n  }\n  // Pooling Layers.\n  /**\n   * Average pooling operation for spatial data.\n   *\n   * Input shape: `[batchSize, inLength, channels]`\n   *\n   * Output shape: `[batchSize, pooledLength, channels]`\n   *\n   * `tf.avgPool1d` is an alias.\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function averagePooling1d(args) {\n    return new _layersPooling.AveragePooling1D(args);\n  }\n  function avgPool1d(args) {\n    return averagePooling1d(args);\n  }\n  // For backwards compatibility.\n  // See https://github.com/tensorflow/tfjs/issues/152\n  function avgPooling1d(args) {\n    return averagePooling1d(args);\n  }\n  /**\n   * Average pooling operation for spatial data.\n   *\n   * Input shape:\n   *  - If `dataFormat === CHANNEL_LAST`:\n   *      4D tensor with shape:\n   *      `[batchSize, rows, cols, channels]`\n   *  - If `dataFormat === CHANNEL_FIRST`:\n   *      4D tensor with shape:\n   *      `[batchSize, channels, rows, cols]`\n   *\n   * Output shape\n   *  - If `dataFormat === CHANNEL_LAST`:\n   *      4D tensor with shape:\n   *      `[batchSize, pooledRows, pooledCols, channels]`\n   *  - If `dataFormat === CHANNEL_FIRST`:\n   *      4D tensor with shape:\n   *      `[batchSize, channels, pooledRows, pooledCols]`\n   *\n   * `tf.avgPool2d` is an alias.\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function averagePooling2d(args) {\n    return new _layersPooling.AveragePooling2D(args);\n  }\n  function avgPool2d(args) {\n    return averagePooling2d(args);\n  }\n  // For backwards compatibility.\n  // See https://github.com/tensorflow/tfjs/issues/152\n  function avgPooling2d(args) {\n    return averagePooling2d(args);\n  }\n  /**\n   * Average pooling operation for 3D data.\n   *\n   * Input shape\n   *   - If `dataFormat === channelsLast`:\n   *       5D tensor with shape:\n   *       `[batchSize, depths, rows, cols, channels]`\n   *   - If `dataFormat === channelsFirst`:\n   *      4D tensor with shape:\n   *       `[batchSize, channels, depths, rows, cols]`\n   *\n   * Output shape\n   *   - If `dataFormat=channelsLast`:\n   *       5D tensor with shape:\n   *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n   *   - If `dataFormat=channelsFirst`:\n   *       5D tensor with shape:\n   *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function averagePooling3d(args) {\n    return new _layersPooling.AveragePooling3D(args);\n  }\n  function avgPool3d(args) {\n    return averagePooling3d(args);\n  }\n  // For backwards compatibility.\n  // See https://github.com/tensorflow/tfjs/issues/152\n  function avgPooling3d(args) {\n    return averagePooling3d(args);\n  }\n  /**\n   * Global average pooling operation for temporal data.\n   *\n   * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n   *\n   * Output Shape: 2D tensor with shape: `[batchSize, features]`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function globalAveragePooling1d(args) {\n    return new _layersPooling.GlobalAveragePooling1D(args);\n  }\n  /**\n   * Global average pooling operation for spatial data.\n   *\n   * Input shape:\n   *   - If `dataFormat` is `CHANNEL_LAST`:\n   *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n   *   - If `dataFormat` is `CHANNEL_FIRST`:\n   *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n   *\n   * Output shape:\n   *   2D tensor with shape: `[batchSize, channels]`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function globalAveragePooling2d(args) {\n    return new _layersPooling.GlobalAveragePooling2D(args);\n  }\n  /**\n   * Global max pooling operation for temporal data.\n   *\n   * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n   *\n   * Output Shape: 2D tensor with shape: `[batchSize, features]`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function globalMaxPooling1d(args) {\n    return new _layersPooling.GlobalMaxPooling1D(args);\n  }\n  /**\n   * Global max pooling operation for spatial data.\n   *\n   * Input shape:\n   *   - If `dataFormat` is `CHANNEL_LAST`:\n   *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n   *   - If `dataFormat` is `CHANNEL_FIRST`:\n   *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n   *\n   * Output shape:\n   *   2D tensor with shape: `[batchSize, channels]`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function globalMaxPooling2d(args) {\n    return new _layersPooling.GlobalMaxPooling2D(args);\n  }\n  /**\n   * Max pooling operation for temporal data.\n   *\n   * Input shape:  `[batchSize, inLength, channels]`\n   *\n   * Output shape: `[batchSize, pooledLength, channels]`\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function maxPooling1d(args) {\n    return new _layersPooling.MaxPooling1D(args);\n  }\n  /**\n   * Max pooling operation for spatial data.\n   *\n   * Input shape\n   *   - If `dataFormat === CHANNEL_LAST`:\n   *       4D tensor with shape:\n   *       `[batchSize, rows, cols, channels]`\n   *   - If `dataFormat === CHANNEL_FIRST`:\n   *      4D tensor with shape:\n   *       `[batchSize, channels, rows, cols]`\n   *\n   * Output shape\n   *   - If `dataFormat=CHANNEL_LAST`:\n   *       4D tensor with shape:\n   *       `[batchSize, pooledRows, pooledCols, channels]`\n   *   - If `dataFormat=CHANNEL_FIRST`:\n   *       4D tensor with shape:\n   *       `[batchSize, channels, pooledRows, pooledCols]`\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function maxPooling2d(args) {\n    return new _layersPooling.MaxPooling2D(args);\n  }\n  /**\n   * Max pooling operation for 3D data.\n   *\n   * Input shape\n   *   - If `dataFormat === channelsLast`:\n   *       5D tensor with shape:\n   *       `[batchSize, depths, rows, cols, channels]`\n   *   - If `dataFormat === channelsFirst`:\n   *      5D tensor with shape:\n   *       `[batchSize, channels, depths, rows, cols]`\n   *\n   * Output shape\n   *   - If `dataFormat=channelsLast`:\n   *       5D tensor with shape:\n   *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n   *   - If `dataFormat=channelsFirst`:\n   *       5D tensor with shape:\n   *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n   *\n   * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n   */\n  function maxPooling3d(args) {\n    return new _layersPooling.MaxPooling3D(args);\n  }\n  // Recurrent Layers.\n  /**\n   * Gated Recurrent Unit - Cho et al. 2014.\n   *\n   * This is an `RNN` layer consisting of one `GRUCell`. However, unlike\n   * the underlying `GRUCell`, the `apply` method of `SimpleRNN` operates\n   * on a sequence of inputs. The shape of the input (not including the first,\n   * batch dimension) needs to be at least 2-D, with the first dimension being\n   * time steps. For example:\n   *\n   * ```js\n   * const rnn = tf.layers.gru({units: 8, returnSequences: true});\n   *\n   * // Create an input with 10 time steps.\n   * const input = tf.input({shape: [10, 20]});\n   * const output = rnn.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n   * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n   * // 3rd dimension is the `GRUCell`'s number of units.\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function gru(args) {\n    return new _layersRecurrent.GRU(args);\n  }\n  /**\n   * Cell class for `GRU`.\n   *\n   * `GRUCell` is distinct from the `RNN` subclass `GRU` in that its\n   * `apply` method takes the input data of only a single time step and returns\n   * the cell's output at the time step, while `GRU` takes the input data\n   * over a number of time steps. For example:\n   *\n   * ```js\n   * const cell = tf.layers.gruCell({units: 2});\n   * const input = tf.input({shape: [10]});\n   * const output = cell.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10]: This is the cell's output at a single time step. The 1st\n   * // dimension is the unknown batch size.\n   * ```\n   *\n   * Instance(s) of `GRUCell` can be used to construct `RNN` layers. The\n   * most typical use of this workflow is to combine a number of cells into a\n   * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n   * RNN. For example:\n   *\n   * ```js\n   * const cells = [\n   *   tf.layers.gruCell({units: 4}),\n   *   tf.layers.gruCell({units: 8}),\n   * ];\n   * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n   *\n   * // Create an input with 10 time steps and a length-20 vector at each step.\n   * const input = tf.input({shape: [10, 20]});\n   * const output = rnn.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n   * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n   * // 3rd dimension is the last `gruCell`'s number of units.\n   * ```\n   *\n   * To create an `RNN` consisting of only *one* `GRUCell`, use the\n   * `tf.layers.gru`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function gruCell(args) {\n    return new _layersRecurrent.GRUCell(args);\n  }\n  /**\n   * Long-Short Term Memory layer - Hochreiter 1997.\n   *\n   * This is an `RNN` layer consisting of one `LSTMCell`. However, unlike\n   * the underlying `LSTMCell`, the `apply` method of `LSTM` operates\n   * on a sequence of inputs. The shape of the input (not including the first,\n   * batch dimension) needs to be at least 2-D, with the first dimension being\n   * time steps. For example:\n   *\n   * ```js\n   * const lstm = tf.layers.lstm({units: 8, returnSequences: true});\n   *\n   * // Create an input with 10 time steps.\n   * const input = tf.input({shape: [10, 20]});\n   * const output = lstm.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n   * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n   * // 3rd dimension is the `LSTMCell`'s number of units.\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function lstm(args) {\n    return new _layersRecurrent.LSTM(args);\n  }\n  /**\n   * Cell class for `LSTM`.\n   *\n   * `LSTMCell` is distinct from the `RNN` subclass `LSTM` in that its\n   * `apply` method takes the input data of only a single time step and returns\n   * the cell's output at the time step, while `LSTM` takes the input data\n   * over a number of time steps. For example:\n   *\n   * ```js\n   * const cell = tf.layers.lstmCell({units: 2});\n   * const input = tf.input({shape: [10]});\n   * const output = cell.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10]: This is the cell's output at a single time step. The 1st\n   * // dimension is the unknown batch size.\n   * ```\n   *\n   * Instance(s) of `LSTMCell` can be used to construct `RNN` layers. The\n   * most typical use of this workflow is to combine a number of cells into a\n   * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n   * RNN. For example:\n   *\n   * ```js\n   * const cells = [\n   *   tf.layers.lstmCell({units: 4}),\n   *   tf.layers.lstmCell({units: 8}),\n   * ];\n   * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n   *\n   * // Create an input with 10 time steps and a length-20 vector at each step.\n   * const input = tf.input({shape: [10, 20]});\n   * const output = rnn.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n   * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n   * // 3rd dimension is the last `lstmCell`'s number of units.\n   * ```\n   *\n   * To create an `RNN` consisting of only *one* `LSTMCell`, use the\n   * `tf.layers.lstm`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function lstmCell(args) {\n    return new _layersRecurrent.LSTMCell(args);\n  }\n  /**\n   * Fully-connected RNN where the output is to be fed back to input.\n   *\n   * This is an `RNN` layer consisting of one `SimpleRNNCell`. However, unlike\n   * the underlying `SimpleRNNCell`, the `apply` method of `SimpleRNN` operates\n   * on a sequence of inputs. The shape of the input (not including the first,\n   * batch dimension) needs to be at least 2-D, with the first dimension being\n   * time steps. For example:\n   *\n   * ```js\n   * const rnn = tf.layers.simpleRNN({units: 8, returnSequences: true});\n   *\n   * // Create an input with 10 time steps.\n   * const input = tf.input({shape: [10, 20]});\n   * const output = rnn.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n   * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n   * // 3rd dimension is the `SimpleRNNCell`'s number of units.\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function simpleRNN(args) {\n    return new _layersRecurrent.SimpleRNN(args);\n  }\n  /**\n   * Cell class for `SimpleRNN`.\n   *\n   * `SimpleRNNCell` is distinct from the `RNN` subclass `SimpleRNN` in that its\n   * `apply` method takes the input data of only a single time step and returns\n   * the cell's output at the time step, while `SimpleRNN` takes the input data\n   * over a number of time steps. For example:\n   *\n   * ```js\n   * const cell = tf.layers.simpleRNNCell({units: 2});\n   * const input = tf.input({shape: [10]});\n   * const output = cell.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10]: This is the cell's output at a single time step. The 1st\n   * // dimension is the unknown batch size.\n   * ```\n   *\n   * Instance(s) of `SimpleRNNCell` can be used to construct `RNN` layers. The\n   * most typical use of this workflow is to combine a number of cells into a\n   * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n   * RNN. For example:\n   *\n   * ```js\n   * const cells = [\n   *   tf.layers.simpleRNNCell({units: 4}),\n   *   tf.layers.simpleRNNCell({units: 8}),\n   * ];\n   * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n   *\n   * // Create an input with 10 time steps and a length-20 vector at each step.\n   * const input = tf.input({shape: [10, 20]});\n   * const output = rnn.apply(input);\n   *\n   * console.log(JSON.stringify(output.shape));\n   * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n   * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n   * // 3rd dimension is the last `SimpleRNNCell`'s number of units.\n   * ```\n   *\n   * To create an `RNN` consisting of only *one* `SimpleRNNCell`, use the\n   * `tf.layers.simpleRNN`.\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function simpleRNNCell(args) {\n    return new _layersRecurrent.SimpleRNNCell(args);\n  }\n  /**\n   * Convolutional LSTM layer - Xingjian Shi 2015.\n   *\n   * This is a `ConvRNN2D` layer consisting of one `ConvLSTM2DCell`. However,\n   * unlike the underlying `ConvLSTM2DCell`, the `apply` method of `ConvLSTM2D`\n   * operates on a sequence of inputs. The shape of the input (not including the\n   * first, batch dimension) needs to be 4-D, with the first dimension being time\n   * steps. For example:\n   *\n   * ```js\n   * const filters = 3;\n   * const kernelSize = 3;\n   *\n   * const batchSize = 4;\n   * const sequenceLength = 2;\n   * const size = 5;\n   * const channels = 3;\n   *\n   * const inputShape = [batchSize, sequenceLength, size, size, channels];\n   * const input = tf.ones(inputShape);\n   *\n   * const layer = tf.layers.convLstm2d({filters, kernelSize});\n   *\n   * const output = layer.apply(input);\n   * ```\n   */\n  /** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\n  function convLstm2d(args) {\n    return new _layersConvolutional_recurrent.ConvLSTM2D(args);\n  }\n  /**\n   * Cell class for `ConvLSTM2D`.\n   *\n   * `ConvLSTM2DCell` is distinct from the `ConvRNN2D` subclass `ConvLSTM2D` in\n   * that its `call` method takes the input data of only a single time step and\n   * returns the cell's output at the time step, while `ConvLSTM2D` takes the\n   * input data over a number of time steps. For example:\n   *\n   * ```js\n   * const filters = 3;\n   * const kernelSize = 3;\n   *\n   * const sequenceLength = 1;\n   * const size = 5;\n   * const channels = 3;\n   *\n   * const inputShape = [sequenceLength, size, size, channels];\n   * const input = tf.ones(inputShape);\n   *\n   * const cell = tf.layers.convLstm2dCell({filters, kernelSize});\n   *\n   * cell.build(input.shape);\n   *\n   * const outputSize = size - kernelSize + 1;\n   * const outShape = [sequenceLength, outputSize, outputSize, filters];\n   *\n   * const initialH = tf.zeros(outShape);\n   * const initialC = tf.zeros(outShape);\n   *\n   * const [o, h, c] = cell.call([input, initialH, initialC], {});\n   * ```\n   */\n  /** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\n  function convLstm2dCell(args) {\n    return new _layersConvolutional_recurrent.ConvLSTM2DCell(args);\n  }\n  /**\n   * Base class for recurrent layers.\n   *\n   * Input shape:\n   *   3D tensor with shape `[batchSize, timeSteps, inputDim]`.\n   *\n   * Output shape:\n   *   - if `returnState`, an Array of tensors (i.e., `tf.Tensor`s). The first\n   *     tensor is the output. The remaining tensors are the states at the\n   *     last time step, each with shape `[batchSize, units]`.\n   *   - if `returnSequences`, the output will have shape\n   *     `[batchSize, timeSteps, units]`.\n   *   - else, the output will have shape `[batchSize, units]`.\n   *\n   * Masking:\n   *   This layer supports masking for input data with a variable number\n   *   of timesteps. To introduce masks to your data,\n   *   use an embedding layer with the `mask_zero` parameter\n   *   set to `True`.\n   *\n   * Notes on using statefulness in RNNs:\n   *   You can set RNN layers to be 'stateful', which means that the states\n   *   computed for the samples in one batch will be reused as initial states\n   *   for the samples in the next batch. This assumes a one-to-one mapping\n   *   between samples in different successive batches.\n   *\n   *   To enable statefulness:\n   *     - specify `stateful: true` in the layer constructor.\n   *     - specify a fixed batch size for your model, by passing\n   *       if sequential model:\n   *         `batchInputShape=[...]` to the first layer in your model.\n   *       else for functional model with 1 or more Input layers:\n   *         `batchShape=[...]` to all the first layers in your model.\n   *       This is the expected shape of your inputs *including the batch size*.\n   *       It should be a tuple of integers, e.g. `(32, 10, 100)`.\n   *     - specify `shuffle=False` when calling fit().\n   *\n   *   To reset the states of your model, call `.resetStates()` on either\n   *   a specific layer, or on your entire model.\n   *\n   * Note on specifying the initial state of RNNs\n   *   You can specify the initial state of RNN layers symbolically by\n   *   calling them with the option `initialState`. The value of\n   *   `initialState` should be a tensor or list of tensors representing\n   *   the initial state of the RNN layer.\n   *\n   *   You can specify the initial state of RNN layers numerically by\n   *   calling `resetStates` with the keyword argument `states`. The value of\n   *   `states` should be a numpy array or list of numpy arrays representing\n   *   the initial state of the RNN layer.\n   *\n   * Note on passing external constants to RNNs\n   *   You can pass \"external\" constants to the cell using the `constants`\n   *   keyword argument of `RNN.call` method. This requires that the `cell.call`\n   *   method accepts the same keyword argument `constants`. Such constants\n   *   can be used to condition the cell transformation on additional static\n   *   inputs (not changing over time), a.k.a. an attention mechanism.\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function rnn(args) {\n    return new _layersRecurrent.RNN(args);\n  }\n  /**\n   * Wrapper allowing a stack of RNN cells to behave as a single cell.\n   *\n   * Used to implement efficient stacked RNNs.\n   *\n   * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n   */\n  function stackedRNNCells(args) {\n    return new _layersRecurrent.StackedRNNCells(args);\n  }\n  // Wrapper Layers.\n  /** @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'} */\n  function bidirectional(args) {\n    return new _layersWrappers.Bidirectional(args);\n  }\n  /**\n   * This wrapper applies a layer to every temporal slice of an input.\n   *\n   * The input should be at least 3D,  and the dimension of the index `1` will be\n   * considered to be the temporal dimension.\n   *\n   * Consider a batch of 32 samples, where each sample is a sequence of 10 vectors\n   * of 16 dimensions. The batch input shape of the layer is then `[32,  10,\n   * 16]`, and the `inputShape`, not including the sample dimension, is\n   * `[10, 16]`.\n   *\n   * You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10\n   * timesteps, independently:\n   *\n   * ```js\n   * const model = tf.sequential();\n   * model.add(tf.layers.timeDistributed({\n   *   layer: tf.layers.dense({units: 8}),\n   *   inputShape: [10, 16],\n   * }));\n   *\n   * // Now model.outputShape = [null, 10, 8].\n   * // The output will then have shape `[32, 10, 8]`.\n   *\n   * // In subsequent layers, there is no need for `inputShape`:\n   * model.add(tf.layers.timeDistributed({layer: tf.layers.dense({units: 32})}));\n   * console.log(JSON.stringify(model.outputs[0].shape));\n   * // Now model.outputShape = [null, 10, 32].\n   * ```\n   *\n   * The output will then have shape `[32, 10, 32]`.\n   *\n   * `TimeDistributed` can be used with arbitrary layers, not just `Dense`, for\n   * instance a `Conv2D` layer.\n   *\n   * ```js\n   * const model = tf.sequential();\n   * model.add(tf.layers.timeDistributed({\n   *   layer: tf.layers.conv2d({filters: 64, kernelSize: [3, 3]}),\n   *   inputShape: [10, 299, 299, 3],\n   * }));\n   * console.log(JSON.stringify(model.outputs[0].shape));\n   * ```\n   *\n   * @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'}\n   */\n  function timeDistributed(args) {\n    return new _layersWrappers.TimeDistributed(args);\n  }\n  // Aliases for pooling.\n  const globalMaxPool1d = globalMaxPooling1d;\n  const globalMaxPool2d = globalMaxPooling2d;\n  const maxPool1d = maxPooling1d;\n  const maxPool2d = maxPooling2d;\n  /**\n   * Apply additive zero-centered Gaussian noise.\n   *\n   * As it is a regularization layer, it is only active at training time.\n   *\n   * This is useful to mitigate overfitting\n   * (you could see it as a form of random data augmentation).\n   * Gaussian Noise (GS) is a natural choice as corruption process\n   * for real valued inputs.\n   *\n   * # Arguments\n   * stddev: float, standard deviation of the noise distribution.\n   *\n   * # Input shape\n   * Arbitrary. Use the keyword argument `input_shape`\n   * (tuple of integers, does not include the samples axis)\n   * when using this layer as the first layer in a model.\n   *\n   * # Output shape\n   * Same shape as input.\n   *\n   * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\n   */\n  function gaussianNoise(args) {\n    return new _layersNoise.GaussianNoise(args);\n  }\n  /**\n   * Apply multiplicative 1-centered Gaussian noise.\n   *\n   * As it is a regularization layer, it is only active at training time.\n   *\n   * Arguments:\n   *   - `rate`: float, drop probability (as with `Dropout`).\n   *     The multiplicative noise will have\n   *     standard deviation `sqrt(rate / (1 - rate))`.\n   *\n   * Input shape:\n   *   Arbitrary. Use the keyword argument `inputShape`\n   *   (tuple of integers, does not include the samples axis)\n   *   when using this layer as the first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as input.\n   *\n   * References:\n   *   - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n   *      http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n   *\n   * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\n   */\n  function gaussianDropout(args) {\n    return new _layersNoise.GaussianDropout(args);\n  }\n  /**\n   * Applies Alpha Dropout to the input.\n   *\n   * As it is a regularization layer, it is only active at training time.\n   *\n   * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n   * to their original values, in order to ensure the self-normalizing property\n   * even after this dropout.\n   * Alpha Dropout fits well to Scaled Exponential Linear Units\n   * by randomly setting activations to the negative saturation value.\n   *\n   * Arguments:\n   *   - `rate`: float, drop probability (as with `Dropout`).\n   *     The multiplicative noise will have\n   *     standard deviation `sqrt(rate / (1 - rate))`.\n   *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n   *     shape for randomly generated keep/drop flags.\n   *\n   * Input shape:\n   *   Arbitrary. Use the keyword argument `inputShape`\n   *   (tuple of integers, does not include the samples axis)\n   *   when using this layer as the first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as input.\n   *\n   * References:\n   *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n   *\n   * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\n   */\n  function alphaDropout(args) {\n    return new _layersNoise.AlphaDropout(args);\n  }\n  /**\n   * Masks a sequence by using a mask value to skip timesteps.\n   *\n   * If all features for a given sample timestep are equal to `mask_value`,\n   * then the sample timestep will be masked (skipped) in all downstream layers\n   * (as long as they support masking).\n   *\n   * If any downstream layer does not support masking yet receives such\n   * an input mask, an exception will be raised.\n   *\n   * Arguments:\n   *   - `maskValue`: Either None or mask value to skip.\n   *\n   * Input shape:\n   *   Arbitrary. Use the keyword argument `inputShape`\n   *   (tuple of integers, does not include the samples axis)\n   *   when using this layer as the first layer in a model.\n   *\n   * Output shape:\n   *   Same shape as input.\n   *\n   * @doc {heading: 'Layers', subheading: 'Mask', namespace: 'layers'}\n   */\n  function masking(args) {\n    return new _layersCore.Masking(args);\n  }\n  /**\n   * A preprocessing layer which rescales input values to a new range.\n   *\n   * This layer rescales every value of an input (often an image) by multiplying\n   * by `scale` and adding `offset`.\n   *\n   * For instance:\n   * 1. To rescale an input in the ``[0, 255]`` range\n   * to be in the `[0, 1]` range, you would pass `scale=1/255`.\n   * 2. To rescale an input in the ``[0, 255]`` range to be in the `[-1, 1]`\n   * range, you would pass `scale=1./127.5, offset=-1`.\n   * The rescaling is applied both during training and inference. Inputs can be\n   * of integer or floating point dtype, and by default the layer will output\n   * floats.\n   *\n   * Arguments:\n   *   - `scale`: Float, the scale to apply to the inputs.\n   *   - `offset`: Float, the offset to apply to the inputs.\n   *\n   * Input shape:\n   *   Arbitrary.\n   *\n   * Output shape:\n   *   Same as input.\n   *\n   * @doc {heading: 'Layers', subheading: 'Rescaling', namespace: 'layers'}\n   */\n  function rescaling(args) {\n    return new _layersPreprocessingImage_preprocessing.Rescaling(args);\n  }\n  /**\n   *  A preprocessing layer which center crops images.\n   *\n   *   This layers crops the central portion of the images to a target size. If an\n   *   image is smaller than the target size, it will be resized and cropped so as\n   *   to return the largest possible window in the image that matches the target\n   *   aspect ratio.\n   *\n   *   Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n   *   of integer or floating point dtype.\n   *\n   *   If the input height/width is even and the target height/width is odd (or\n   *   inversely), the input image is left-padded by 1 pixel.\n   *\n   *   Arguments:\n   *     `height`: Integer, the height of the output shape.\n   *     `width`: Integer, the width of the output shape.\n   *\n   *   Input shape:\n   *     3D (unbatched) or 4D (batched) tensor with shape:\n   *     `(..., height, width, channels)`, in `channelsLast` format.\n   *\n   *   Output shape:\n   *     3D (unbatched) or 4D (batched) tensor with shape:\n   *     `(..., targetHeight, targetWidth, channels)`.\n   *\n   *\n   *  @doc {heading: 'Layers', subheading: 'CenterCrop', namespace: 'layers'}\n   */\n  function centerCrop(args) {\n    return new _layersPreprocessingCenter_crop.CenterCrop(args);\n  }\n  /**\n   * A preprocessing layer which resizes images.\n   * This layer resizes an image input to a target height and width. The input\n   * should be a 4D (batched) or 3D (unbatched) tensor in `\"channels_last\"`\n   * format.  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0,\n   * 255]`) and of interger or floating point dtype. By default, the layer will\n   * output floats.\n   *\n   * Arguments:\n   *   - `height`: number, the height for the output tensor.\n   *   - `width`: number, the width for the output tensor.\n   *   - `interpolation`: string, the method for image resizing interpolation.\n   *   - `cropToAspectRatio`: boolean, whether to keep image aspect ratio.\n   *\n   * Input shape:\n   *   Arbitrary.\n   *\n   * Output shape:\n   *   height, width, num channels.\n   *\n   * @doc {heading: 'Layers', subheading: 'Resizing', namespace: 'layers'}\n   */\n  function resizing(args) {\n    return new _layersPreprocessingImage_resizing.Resizing(args);\n  }\n  /**\n   * A preprocessing layer which encodes integer features.\n   *\n   * This layer provides options for condensing data into a categorical encoding\n   * when the total number of tokens are known in advance. It accepts integer\n   * values as inputs, and it outputs a dense representation of those\n   * inputs.\n   *\n   * Arguments:\n   *\n   * numTokens: The total number of tokens the layer should support. All\n   *  inputs to the layer must integers in the range `0 <= value <\n   *  numTokens`, or an error will be thrown.\n   *\n   * outputMode: Specification for the output of the layer.\n   *  Defaults to `multiHot`. Values can be `oneHot`, `multiHot` or\n   *  `count`, configuring the layer as follows:\n   *\n   *    oneHot: Encodes each individual element in the input into an\n   *      array of `numTokens` size, containing a 1 at the element index. If\n   *      the last dimension is size 1, will encode on that dimension. If the\n   *      last dimension is not size 1, will append a new dimension for the\n   *      encoded output.\n   *\n   *    multiHot: Encodes each sample in the input into a single array\n   *     of `numTokens` size, containing a 1 for each vocabulary term\n   *     present in the sample. Treats the last dimension as the sample\n   *     dimension, if input shape is `(..., sampleLength)`, output shape\n   *     will be `(..., numTokens)`.\n   *\n   *    count: Like `multiHot`, but the int array contains a count of\n   *     the number of times the token at that index appeared in the sample.\n   *\n   *  For all output modes, currently only output up to rank 2 is supported.\n   *   Call arguments:\n   *    inputs: A 1D or 2D tensor of integer inputs.\n   *    countWeights: A tensor in the same shape as `inputs` indicating the\n   *    weight for each sample value when summing up in `count` mode. Not used\n   *    in `multiHot` or `oneHot` modes.\n   *\n   *\n   * @doc {heading: 'Layers', subheading: 'CategoryEncoding', namespace: 'layers'}\n   */\n  function categoryEncoding(args) {\n    return new _layersPreprocessingCategory_encoding.CategoryEncoding(args);\n  }\n  /**\n   * A preprocessing layer which randomly varies image width during training.\n   *\n   * This layer will randomly adjusts the width of a batch of images of a batch\n   * of images by a random factor.\n   *\n   * The input should be a 3D (unbatched) or 4D (batched) tensor in\n   * the `\"channels_last\"` image data format. Input pixel values can be of any\n   * range (e.g. `[0., 1.)` or `[0, 255]`) and of integer or floating point\n   * dtype. By default, the layer will output floats. By default, this layer is\n   * inactive during inference. For an overview and full list of preprocessing\n   * layers, see the preprocessing [guide]\n   * (https://www.tensorflow.org/guide/keras/preprocessing_layers).\n   *\n   * Arguments:\n   *\n   * factor:\n   *   A positive float (fraction of original width), or a tuple of size 2\n   *   representing lower and upper bound for resizing vertically.\n   *   When represented as a single float, this value is used for both the upper\n   *   and lower bound. For instance, `factor=(0.2, 0.3)` results in an output\n   *   with width changed by a random amount in the range `[20%, 30%]`.\n   *   `factor=(-0.2, 0.3)` results in an output with width changed by a random\n   *   amount in the range `[-20%, +30%]`. `factor=0.2` results in an output\n   *   with width changed by a random amount in the range `[-20%, +20%]`.\n   * interpolation:\n   *   String, the interpolation method.\n   *   Defaults to `bilinear`.\n   *   Supports `\"bilinear\"`, `\"nearest\"`.\n   *   The tf methods `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`, `\"lanczos5\"`,\n   *   `\"gaussian\"`, `\"mitchellcubic\"` are unimplemented in tfjs.\n   * seed:\n   *   Integer. Used to create a random seed.\n   *\n   * Input shape:\n   *     3D (unbatched) or 4D (batched) tensor with shape:\n   *     `(..., height, width, channels)`, in `\"channels_last\"` format.\n   * Output shape:\n   *     3D (unbatched) or 4D (batched) tensor with shape:\n   *     `(..., height, random_width, channels)`.\n   *\n   *\n   * @doc {heading: 'Layers', subheading: 'RandomWidth', namespace: 'layers'}\n   */\n  function randomWidth(args) {\n    return new _layersPreprocessingRandom_width.RandomWidth(args);\n  }\n});","lineCount":1936,"map":[[7,2,69,0,"exports"],[7,9,69,0],[7,10,69,0,"inputLayer"],[7,20,69,0],[7,23,69,0,"inputLayer"],[7,33,69,0],[8,2,97,0,"exports"],[8,9,97,0],[8,10,97,0,"elu"],[8,13,97,0],[8,16,97,0,"elu"],[8,19,97,0],[9,2,117,0,"exports"],[9,9,117,0],[9,10,117,0,"reLU"],[9,14,117,0],[9,17,117,0,"reLU"],[9,21,117,0],[10,2,140,0,"exports"],[10,9,140,0],[10,10,140,0,"leakyReLU"],[10,19,140,0],[10,22,140,0,"leakyReLU"],[10,31,140,0],[11,2,164,0,"exports"],[11,9,164,0],[11,10,164,0,"prelu"],[11,15,164,0],[11,18,164,0,"prelu"],[11,23,164,0],[12,2,183,0,"exports"],[12,9,183,0],[12,10,183,0,"softmax"],[12,17,183,0],[12,20,183,0,"softmax"],[12,27,183,0],[13,2,210,0,"exports"],[13,9,210,0],[13,10,210,0,"thresholdedReLU"],[13,25,210,0],[13,28,210,0,"thresholdedReLU"],[13,43,210,0],[14,2,234,0,"exports"],[14,9,234,0],[14,10,234,0,"conv1d"],[14,16,234,0],[14,19,234,0,"conv1d"],[14,25,234,0],[15,2,255,0,"exports"],[15,9,255,0],[15,10,255,0,"conv2d"],[15,16,255,0],[15,19,255,0,"conv2d"],[15,25,255,0],[16,2,293,0,"exports"],[16,9,293,0],[16,10,293,0,"conv2dTranspose"],[16,25,293,0],[16,28,293,0,"conv2dTranspose"],[16,43,293,0],[17,2,314,0,"exports"],[17,9,314,0],[17,10,314,0,"conv3d"],[17,16,314,0],[17,19,314,0,"conv3d"],[17,25,314,0],[18,2,317,0,"exports"],[18,9,317,0],[18,10,317,0,"conv3dTranspose"],[18,25,317,0],[18,28,317,0,"conv3dTranspose"],[18,43,317,0],[19,2,349,0,"exports"],[19,9,349,0],[19,10,349,0,"separableConv2d"],[19,25,349,0],[19,28,349,0,"separableConv2d"],[19,43,349,0],[20,2,383,0,"exports"],[20,9,383,0],[20,10,383,0,"cropping2D"],[20,20,383,0],[20,23,383,0,"cropping2D"],[20,33,383,0],[21,2,410,0,"exports"],[21,9,410,0],[21,10,410,0,"upSampling2d"],[21,22,410,0],[21,25,410,0,"upSampling2d"],[21,37,410,0],[22,2,424,0,"exports"],[22,9,424,0],[22,10,424,0,"depthwiseConv2d"],[22,25,424,0],[22,28,424,0,"depthwiseConv2d"],[22,43,424,0],[23,2,459,0,"exports"],[23,9,459,0],[23,10,459,0,"activation"],[23,20,459,0],[23,23,459,0,"activation"],[23,33,459,0],[24,2,495,0,"exports"],[24,9,495,0],[24,10,495,0,"dense"],[24,15,495,0],[24,18,495,0,"dense"],[24,23,495,0],[25,2,508,0,"exports"],[25,9,508,0],[25,10,508,0,"dropout"],[25,17,508,0],[25,20,508,0,"dropout"],[25,27,508,0],[26,2,543,0,"exports"],[26,9,543,0],[26,10,543,0,"spatialDropout1d"],[26,26,543,0],[26,29,543,0,"spatialDropout1d"],[26,45,543,0],[27,2,565,0,"exports"],[27,9,565,0],[27,10,565,0,"flatten"],[27,17,565,0],[27,20,565,0,"flatten"],[27,27,565,0],[28,2,582,0,"exports"],[28,9,582,0],[28,10,582,0,"repeatVector"],[28,22,582,0],[28,25,582,0,"repeatVector"],[28,37,582,0],[29,2,608,0,"exports"],[29,9,608,0],[29,10,608,0,"reshape"],[29,17,608,0],[29,20,608,0,"reshape"],[29,27,608,0],[30,2,639,0,"exports"],[30,9,639,0],[30,10,639,0,"permute"],[30,17,639,0],[30,20,639,0,"permute"],[30,27,639,0],[31,2,653,0,"exports"],[31,9,653,0],[31,10,653,0,"embedding"],[31,19,653,0],[31,22,653,0,"embedding"],[31,31,653,0],[32,2,677,0,"exports"],[32,9,677,0],[32,10,677,0,"add"],[32,13,677,0],[32,16,677,0,"add"],[32,19,677,0],[33,2,698,0,"exports"],[33,9,698,0],[33,10,698,0,"average"],[33,17,698,0],[33,20,698,0,"average"],[33,27,698,0],[34,2,721,0,"exports"],[34,9,721,0],[34,10,721,0,"concatenate"],[34,21,721,0],[34,24,721,0,"concatenate"],[34,35,721,0],[35,2,742,0,"exports"],[35,9,742,0],[35,10,742,0,"maximum"],[35,17,742,0],[35,20,742,0,"maximum"],[35,27,742,0],[36,2,763,0,"exports"],[36,9,763,0],[36,10,763,0,"minimum"],[36,17,763,0],[36,20,763,0,"minimum"],[36,27,763,0],[37,2,785,0,"exports"],[37,9,785,0],[37,10,785,0,"multiply"],[37,18,785,0],[37,21,785,0,"multiply"],[37,29,785,0],[38,2,810,0,"exports"],[38,9,810,0],[38,10,810,0,"dot"],[38,13,810,0],[38,16,810,0,"dot"],[38,19,810,0],[39,2,835,0,"exports"],[39,9,835,0],[39,10,835,0,"batchNormalization"],[39,28,835,0],[39,31,835,0,"batchNormalization"],[39,49,835,0],[40,2,858,0,"exports"],[40,9,858,0],[40,10,858,0,"layerNormalization"],[40,28,858,0],[40,31,858,0,"layerNormalization"],[40,49,858,0],[41,2,884,0,"exports"],[41,9,884,0],[41,10,884,0,"zeroPadding2d"],[41,23,884,0],[41,26,884,0,"zeroPadding2d"],[41,39,884,0],[42,2,899,0,"exports"],[42,9,899,0],[42,10,899,0,"averagePooling1d"],[42,26,899,0],[42,29,899,0,"averagePooling1d"],[42,45,899,0],[43,2,902,0,"exports"],[43,9,902,0],[43,10,902,0,"avgPool1d"],[43,19,902,0],[43,22,902,0,"avgPool1d"],[43,31,902,0],[44,2,907,0,"exports"],[44,9,907,0],[44,10,907,0,"avgPooling1d"],[44,22,907,0],[44,25,907,0,"avgPooling1d"],[44,37,907,0],[45,2,933,0,"exports"],[45,9,933,0],[45,10,933,0,"averagePooling2d"],[45,26,933,0],[45,29,933,0,"averagePooling2d"],[45,45,933,0],[46,2,936,0,"exports"],[46,9,936,0],[46,10,936,0,"avgPool2d"],[46,19,936,0],[46,22,936,0,"avgPool2d"],[46,31,936,0],[47,2,941,0,"exports"],[47,9,941,0],[47,10,941,0,"avgPooling2d"],[47,22,941,0],[47,25,941,0,"avgPooling2d"],[47,37,941,0],[48,2,965,0,"exports"],[48,9,965,0],[48,10,965,0,"averagePooling3d"],[48,26,965,0],[48,29,965,0,"averagePooling3d"],[48,45,965,0],[49,2,968,0,"exports"],[49,9,968,0],[49,10,968,0,"avgPool3d"],[49,19,968,0],[49,22,968,0,"avgPool3d"],[49,31,968,0],[50,2,973,0,"exports"],[50,9,973,0],[50,10,973,0,"avgPooling3d"],[50,22,973,0],[50,25,973,0,"avgPooling3d"],[50,37,973,0],[51,2,985,0,"exports"],[51,9,985,0],[51,10,985,0,"globalAveragePooling1d"],[51,32,985,0],[51,35,985,0,"globalAveragePooling1d"],[51,57,985,0],[52,2,1002,0,"exports"],[52,9,1002,0],[52,10,1002,0,"globalAveragePooling2d"],[52,32,1002,0],[52,35,1002,0,"globalAveragePooling2d"],[52,57,1002,0],[53,2,1014,0,"exports"],[53,9,1014,0],[53,10,1014,0,"globalMaxPooling1d"],[53,28,1014,0],[53,31,1014,0,"globalMaxPooling1d"],[53,49,1014,0],[54,2,1031,0,"exports"],[54,9,1031,0],[54,10,1031,0,"globalMaxPooling2d"],[54,28,1031,0],[54,31,1031,0,"globalMaxPooling2d"],[54,49,1031,0],[55,2,1043,0,"exports"],[55,9,1043,0],[55,10,1043,0,"maxPooling1d"],[55,22,1043,0],[55,25,1043,0,"maxPooling1d"],[55,37,1043,0],[56,2,1067,0,"exports"],[56,9,1067,0],[56,10,1067,0,"maxPooling2d"],[56,22,1067,0],[56,25,1067,0,"maxPooling2d"],[56,37,1067,0],[57,2,1091,0,"exports"],[57,9,1091,0],[57,10,1091,0,"maxPooling3d"],[57,22,1091,0],[57,25,1091,0,"maxPooling3d"],[57,37,1091,0],[58,2,1118,0,"exports"],[58,9,1118,0],[58,10,1118,0,"gru"],[58,13,1118,0],[58,16,1118,0,"gru"],[58,19,1118,0],[59,2,1166,0,"exports"],[59,9,1166,0],[59,10,1166,0,"gruCell"],[59,17,1166,0],[59,20,1166,0,"gruCell"],[59,27,1166,0],[60,2,1192,0,"exports"],[60,9,1192,0],[60,10,1192,0,"lstm"],[60,14,1192,0],[60,17,1192,0,"lstm"],[60,21,1192,0],[61,2,1240,0,"exports"],[61,9,1240,0],[61,10,1240,0,"lstmCell"],[61,18,1240,0],[61,21,1240,0,"lstmCell"],[61,29,1240,0],[62,2,1267,0,"exports"],[62,9,1267,0],[62,10,1267,0,"simpleRNN"],[62,19,1267,0],[62,22,1267,0,"simpleRNN"],[62,31,1267,0],[63,2,1315,0,"exports"],[63,9,1315,0],[63,10,1315,0,"simpleRNNCell"],[63,23,1315,0],[63,26,1315,0,"simpleRNNCell"],[63,39,1315,0],[64,2,1345,0,"exports"],[64,9,1345,0],[64,10,1345,0,"convLstm2d"],[64,20,1345,0],[64,23,1345,0,"convLstm2d"],[64,33,1345,0],[65,2,1381,0,"exports"],[65,9,1381,0],[65,10,1381,0,"convLstm2dCell"],[65,24,1381,0],[65,27,1381,0,"convLstm2dCell"],[65,41,1381,0],[66,2,1444,0,"exports"],[66,9,1444,0],[66,10,1444,0,"rnn"],[66,13,1444,0],[66,16,1444,0,"rnn"],[66,19,1444,0],[67,2,1454,0,"exports"],[67,9,1454,0],[67,10,1454,0,"stackedRNNCells"],[67,25,1454,0],[67,28,1454,0,"stackedRNNCells"],[67,43,1454,0],[68,2,1459,0,"exports"],[68,9,1459,0],[68,10,1459,0,"bidirectional"],[68,23,1459,0],[68,26,1459,0,"bidirectional"],[68,39,1459,0],[69,2,1508,0,"exports"],[69,9,1508,0],[69,10,1508,0,"timeDistributed"],[69,25,1508,0],[69,28,1508,0,"timeDistributed"],[69,43,1508,0],[70,2,1512,0,"Object"],[70,8,1512,0],[70,9,1512,0,"defineProperty"],[70,23,1512,0],[70,24,1512,0,"exports"],[70,31,1512,0],[71,4,1512,0,"enumerable"],[71,14,1512,0],[72,4,1512,0,"get"],[72,7,1512,0],[72,18,1512,0,"get"],[72,19,1512,0],[73,6,1512,0],[73,13,1512,0,"globalMaxPool1d"],[73,28,1512,0],[74,4,1512,0],[75,2,1512,0],[76,2,1513,0,"Object"],[76,8,1513,0],[76,9,1513,0,"defineProperty"],[76,23,1513,0],[76,24,1513,0,"exports"],[76,31,1513,0],[77,4,1513,0,"enumerable"],[77,14,1513,0],[78,4,1513,0,"get"],[78,7,1513,0],[78,18,1513,0,"get"],[78,19,1513,0],[79,6,1513,0],[79,13,1513,0,"globalMaxPool2d"],[79,28,1513,0],[80,4,1513,0],[81,2,1513,0],[82,2,1514,0,"Object"],[82,8,1514,0],[82,9,1514,0,"defineProperty"],[82,23,1514,0],[82,24,1514,0,"exports"],[82,31,1514,0],[83,4,1514,0,"enumerable"],[83,14,1514,0],[84,4,1514,0,"get"],[84,7,1514,0],[84,18,1514,0,"get"],[84,19,1514,0],[85,6,1514,0],[85,13,1514,0,"maxPool1d"],[85,22,1514,0],[86,4,1514,0],[87,2,1514,0],[88,2,1515,0,"Object"],[88,8,1515,0],[88,9,1515,0,"defineProperty"],[88,23,1515,0],[88,24,1515,0,"exports"],[88,31,1515,0],[89,4,1515,0,"enumerable"],[89,14,1515,0],[90,4,1515,0,"get"],[90,7,1515,0],[90,18,1515,0,"get"],[90,19,1515,0],[91,6,1515,0],[91,13,1515,0,"maxPool2d"],[91,22,1515,0],[92,4,1515,0],[93,2,1515,0],[94,2,1516,0,"Object"],[94,8,1516,0],[94,9,1516,0,"defineProperty"],[94,23,1516,0],[94,24,1516,0,"exports"],[94,31,1516,0],[95,4,1516,0,"enumerable"],[95,14,1516,0],[96,4,1516,0,"get"],[96,7,1516,0],[96,18,1516,0,"get"],[96,19,1516,0],[97,6,1516,0],[97,13,1516,9,"Layer"],[97,28,1516,14],[97,29,1516,14,"Layer"],[97,34,1516,14],[98,4,1516,14],[99,2,1516,14],[100,2,1516,0,"Object"],[100,8,1516,0],[100,9,1516,0,"defineProperty"],[100,23,1516,0],[100,24,1516,0,"exports"],[100,31,1516,0],[101,4,1516,0,"enumerable"],[101,14,1516,0],[102,4,1516,0,"get"],[102,7,1516,0],[102,18,1516,0,"get"],[102,19,1516,0],[103,6,1516,0],[103,13,1516,16,"RNN"],[103,29,1516,19],[103,30,1516,19,"RNN"],[103,33,1516,19],[104,4,1516,19],[105,2,1516,19],[106,2,1516,0,"Object"],[106,8,1516,0],[106,9,1516,0,"defineProperty"],[106,23,1516,0],[106,24,1516,0,"exports"],[106,31,1516,0],[107,4,1516,0,"enumerable"],[107,14,1516,0],[108,4,1516,0,"get"],[108,7,1516,0],[108,18,1516,0,"get"],[108,19,1516,0],[109,6,1516,0],[109,13,1516,21,"RNNCell"],[109,29,1516,28],[109,30,1516,28,"RNNCell"],[109,37,1516,28],[110,4,1516,28],[111,2,1516,28],[112,2,1516,0,"Object"],[112,8,1516,0],[112,9,1516,0,"defineProperty"],[112,23,1516,0],[112,24,1516,0,"exports"],[112,31,1516,0],[113,4,1516,0,"enumerable"],[113,14,1516,0],[114,4,1516,0,"get"],[114,7,1516,0],[114,18,1516,0,"get"],[114,19,1516,0],[115,6,1516,0],[115,13,1516,30,"input"],[115,22,1516,35],[115,23,1516,35,"input"],[115,28,1516,35],[116,4,1516,35],[117,2,1516,35],[118,2,1540,0,"exports"],[118,9,1540,0],[118,10,1540,0,"gaussianNoise"],[118,23,1540,0],[118,26,1540,0,"gaussianNoise"],[118,39,1540,0],[119,2,1567,0,"exports"],[119,9,1567,0],[119,10,1567,0,"gaussianDropout"],[119,25,1567,0],[119,28,1567,0,"gaussianDropout"],[119,43,1567,0],[120,2,1601,0,"exports"],[120,9,1601,0],[120,10,1601,0,"alphaDropout"],[120,22,1601,0],[120,25,1601,0,"alphaDropout"],[120,37,1601,0],[121,2,1627,0,"exports"],[121,9,1627,0],[121,10,1627,0,"masking"],[121,17,1627,0],[121,20,1627,0,"masking"],[121,27,1627,0],[122,2,1657,0,"exports"],[122,9,1657,0],[122,10,1657,0,"rescaling"],[122,19,1657,0],[122,22,1657,0,"rescaling"],[122,31,1657,0],[123,2,1689,0,"exports"],[123,9,1689,0],[123,10,1689,0,"centerCrop"],[123,20,1689,0],[123,23,1689,0,"centerCrop"],[123,33,1689,0],[124,2,1714,0,"exports"],[124,9,1714,0],[124,10,1714,0,"resizing"],[124,18,1714,0],[124,21,1714,0,"resizing"],[124,29,1714,0],[125,2,1760,0,"exports"],[125,9,1760,0],[125,10,1760,0,"categoryEncoding"],[125,26,1760,0],[125,29,1760,0,"categoryEncoding"],[125,45,1760,0],[126,2,1807,0,"exports"],[126,9,1807,0],[126,10,1807,0,"randomWidth"],[126,21,1807,0],[126,24,1807,0,"randomWidth"],[126,35,1807,0],[127,2,10,0],[127,6,10,0,"_engineInput_layer"],[127,24,10,0],[127,27,10,0,"require"],[127,34,10,0],[127,35,10,0,"_dependencyMap"],[127,49,10,0],[128,2,11,0],[128,6,11,0,"_engineTopology"],[128,21,11,0],[128,24,11,0,"require"],[128,31,11,0],[128,32,11,0,"_dependencyMap"],[128,46,11,0],[129,2,12,0],[129,6,12,0,"_exports2"],[129,15,12,0],[129,18,12,0,"require"],[129,25,12,0],[129,26,12,0,"_dependencyMap"],[129,40,12,0],[130,2,13,0],[130,6,13,0,"_layersAdvanced_activations"],[130,33,13,0],[130,36,13,0,"require"],[130,43,13,0],[130,44,13,0,"_dependencyMap"],[130,58,13,0],[131,2,14,0],[131,6,14,0,"_layersConvolutional"],[131,26,14,0],[131,29,14,0,"require"],[131,36,14,0],[131,37,14,0,"_dependencyMap"],[131,51,14,0],[132,2,15,0],[132,6,15,0,"_layersConvolutional_depthwise"],[132,36,15,0],[132,39,15,0,"require"],[132,46,15,0],[132,47,15,0,"_dependencyMap"],[132,61,15,0],[133,2,16,0],[133,6,16,0,"_layersConvolutional_recurrent"],[133,36,16,0],[133,39,16,0,"require"],[133,46,16,0],[133,47,16,0,"_dependencyMap"],[133,61,16,0],[134,2,17,0],[134,6,17,0,"_layersCore"],[134,17,17,0],[134,20,17,0,"require"],[134,27,17,0],[134,28,17,0,"_dependencyMap"],[134,42,17,0],[135,2,18,0],[135,6,18,0,"_layersEmbeddings"],[135,23,18,0],[135,26,18,0,"require"],[135,33,18,0],[135,34,18,0,"_dependencyMap"],[135,48,18,0],[136,2,19,0],[136,6,19,0,"_layersMerge"],[136,18,19,0],[136,21,19,0,"require"],[136,28,19,0],[136,29,19,0,"_dependencyMap"],[136,43,19,0],[137,2,20,0],[137,6,20,0,"_layersNoise"],[137,18,20,0],[137,21,20,0,"require"],[137,28,20,0],[137,29,20,0,"_dependencyMap"],[137,43,20,0],[138,2,21,0],[138,6,21,0,"_layersNormalization"],[138,26,21,0],[138,29,21,0,"require"],[138,36,21,0],[138,37,21,0,"_dependencyMap"],[138,51,21,0],[139,2,22,0],[139,6,22,0,"_layersPadding"],[139,20,22,0],[139,23,22,0,"require"],[139,30,22,0],[139,31,22,0,"_dependencyMap"],[139,45,22,0],[140,2,23,0],[140,6,23,0,"_layersPooling"],[140,20,23,0],[140,23,23,0,"require"],[140,30,23,0],[140,31,23,0,"_dependencyMap"],[140,45,23,0],[141,2,24,0],[141,6,24,0,"_layersRecurrent"],[141,22,24,0],[141,25,24,0,"require"],[141,32,24,0],[141,33,24,0,"_dependencyMap"],[141,47,24,0],[142,2,25,0],[142,6,25,0,"_layersWrappers"],[142,21,25,0],[142,24,25,0,"require"],[142,31,25,0],[142,32,25,0,"_dependencyMap"],[142,46,25,0],[143,2,26,0],[143,6,26,0,"_layersPreprocessingImage_preprocessing"],[143,45,26,0],[143,48,26,0,"require"],[143,55,26,0],[143,56,26,0,"_dependencyMap"],[143,70,26,0],[144,2,27,0],[144,6,27,0,"_layersPreprocessingCenter_crop"],[144,37,27,0],[144,40,27,0,"require"],[144,47,27,0],[144,48,27,0,"_dependencyMap"],[144,62,27,0],[145,2,28,0],[145,6,28,0,"_layersPreprocessingCategory_encoding"],[145,43,28,0],[145,46,28,0,"require"],[145,53,28,0],[145,54,28,0,"_dependencyMap"],[145,68,28,0],[146,2,29,0],[146,6,29,0,"_layersPreprocessingImage_resizing"],[146,40,29,0],[146,43,29,0,"require"],[146,50,29,0],[146,51,29,0,"_dependencyMap"],[146,65,29,0],[147,2,30,0],[147,6,30,0,"_layersPreprocessingRandom_width"],[147,38,30,0],[147,41,30,0,"require"],[147,48,30,0],[147,49,30,0,"_dependencyMap"],[147,63,30,0],[148,2,1,0],[149,0,2,0],[150,0,3,0],[151,0,4,0],[152,0,5,0],[153,0,6,0],[154,0,7,0],[155,0,8,0],[156,0,9,0],[158,2,31,0],[159,2,32,0],[160,2,33,0],[161,2,34,0],[162,2,35,0],[163,0,36,0],[164,0,37,0],[165,0,38,0],[166,0,39,0],[167,0,40,0],[168,0,41,0],[169,0,42,0],[170,0,43,0],[171,0,44,0],[172,0,45,0],[173,0,46,0],[174,0,47,0],[175,0,48,0],[176,0,49,0],[177,0,50,0],[178,0,51,0],[179,0,52,0],[180,0,53,0],[181,0,54,0],[182,0,55,0],[183,0,56,0],[184,0,57,0],[185,0,58,0],[186,0,59,0],[187,0,60,0],[188,0,61,0],[189,0,62,0],[190,0,63,0],[191,0,64,0],[192,0,65,0],[193,0,66,0],[194,0,67,0],[195,0,68,0],[196,2,69,7],[196,11,69,16,"inputLayer"],[196,21,69,26,"inputLayer"],[196,22,69,27,"args"],[196,26,69,31],[196,28,69,33],[197,4,70,4],[197,11,70,11],[197,15,70,15,"InputLayer"],[197,33,70,25],[197,34,70,25,"InputLayer"],[197,44,70,25],[197,45,70,26,"args"],[197,49,70,30],[197,50,70,31],[198,2,71,0],[199,2,72,0],[200,2,73,0],[201,0,74,0],[202,0,75,0],[203,0,76,0],[204,0,77,0],[205,0,78,0],[206,0,79,0],[207,0,80,0],[208,0,81,0],[209,0,82,0],[210,0,83,0],[211,0,84,0],[212,0,85,0],[213,0,86,0],[214,0,87,0],[215,0,88,0],[216,0,89,0],[217,0,90,0],[218,0,91,0],[219,0,92,0],[220,0,93,0],[221,0,94,0],[222,0,95,0],[223,0,96,0],[224,2,97,7],[224,11,97,16,"elu"],[224,14,97,19,"elu"],[224,15,97,20,"args"],[224,19,97,24],[224,21,97,26],[225,4,98,4],[225,11,98,11],[225,15,98,15,"ELU"],[225,42,98,18],[225,43,98,18,"ELU"],[225,46,98,18],[225,47,98,19,"args"],[225,51,98,23],[225,52,98,24],[226,2,99,0],[227,2,100,0],[228,0,101,0],[229,0,102,0],[230,0,103,0],[231,0,104,0],[232,0,105,0],[233,0,106,0],[234,0,107,0],[235,0,108,0],[236,0,109,0],[237,0,110,0],[238,0,111,0],[239,0,112,0],[240,0,113,0],[241,0,114,0],[242,0,115,0],[243,0,116,0],[244,2,117,7],[244,11,117,16,"reLU"],[244,15,117,20,"reLU"],[244,16,117,21,"args"],[244,20,117,25],[244,22,117,27],[245,4,118,4],[245,11,118,11],[245,15,118,15,"ReLU"],[245,42,118,19],[245,43,118,19,"ReLU"],[245,47,118,19],[245,48,118,20,"args"],[245,52,118,24],[245,53,118,25],[246,2,119,0],[247,2,120,0],[248,0,121,0],[249,0,122,0],[250,0,123,0],[251,0,124,0],[252,0,125,0],[253,0,126,0],[254,0,127,0],[255,0,128,0],[256,0,129,0],[257,0,130,0],[258,0,131,0],[259,0,132,0],[260,0,133,0],[261,0,134,0],[262,0,135,0],[263,0,136,0],[264,0,137,0],[265,0,138,0],[266,0,139,0],[267,2,140,7],[267,11,140,16,"leakyReLU"],[267,20,140,25,"leakyReLU"],[267,21,140,26,"args"],[267,25,140,30],[267,27,140,32],[268,4,141,4],[268,11,141,11],[268,15,141,15,"LeakyReLU"],[268,42,141,24],[268,43,141,24,"LeakyReLU"],[268,52,141,24],[268,53,141,25,"args"],[268,57,141,29],[268,58,141,30],[269,2,142,0],[270,2,143,0],[271,0,144,0],[272,0,145,0],[273,0,146,0],[274,0,147,0],[275,0,148,0],[276,0,149,0],[277,0,150,0],[278,0,151,0],[279,0,152,0],[280,0,153,0],[281,0,154,0],[282,0,155,0],[283,0,156,0],[284,0,157,0],[285,0,158,0],[286,0,159,0],[287,0,160,0],[288,0,161,0],[289,0,162,0],[290,0,163,0],[291,2,164,7],[291,11,164,16,"prelu"],[291,16,164,21,"prelu"],[291,17,164,22,"args"],[291,21,164,26],[291,23,164,28],[292,4,165,4],[292,11,165,11],[292,15,165,15,"PReLU"],[292,42,165,20],[292,43,165,20,"PReLU"],[292,48,165,20],[292,49,165,21,"args"],[292,53,165,25],[292,54,165,26],[293,2,166,0],[294,2,167,0],[295,0,168,0],[296,0,169,0],[297,0,170,0],[298,0,171,0],[299,0,172,0],[300,0,173,0],[301,0,174,0],[302,0,175,0],[303,0,176,0],[304,0,177,0],[305,0,178,0],[306,0,179,0],[307,0,180,0],[308,0,181,0],[309,0,182,0],[310,2,183,7],[310,11,183,16,"softmax"],[310,18,183,23,"softmax"],[310,19,183,24,"args"],[310,23,183,28],[310,25,183,30],[311,4,184,4],[311,11,184,11],[311,15,184,15,"Softmax"],[311,42,184,22],[311,43,184,22,"Softmax"],[311,50,184,22],[311,51,184,23,"args"],[311,55,184,27],[311,56,184,28],[312,2,185,0],[313,2,186,0],[314,0,187,0],[315,0,188,0],[316,0,189,0],[317,0,190,0],[318,0,191,0],[319,0,192,0],[320,0,193,0],[321,0,194,0],[322,0,195,0],[323,0,196,0],[324,0,197,0],[325,0,198,0],[326,0,199,0],[327,0,200,0],[328,0,201,0],[329,0,202,0],[330,0,203,0],[331,0,204,0],[332,0,205,0],[333,0,206,0],[334,0,207,0],[335,0,208,0],[336,0,209,0],[337,2,210,7],[337,11,210,16,"thresholdedReLU"],[337,26,210,31,"thresholdedReLU"],[337,27,210,32,"args"],[337,31,210,36],[337,33,210,38],[338,4,211,4],[338,11,211,11],[338,15,211,15,"ThresholdedReLU"],[338,42,211,30],[338,43,211,30,"ThresholdedReLU"],[338,58,211,30],[338,59,211,31,"args"],[338,63,211,35],[338,64,211,36],[339,2,212,0],[340,2,213,0],[341,2,214,0],[342,0,215,0],[343,0,216,0],[344,0,217,0],[345,0,218,0],[346,0,219,0],[347,0,220,0],[348,0,221,0],[349,0,222,0],[350,0,223,0],[351,0,224,0],[352,0,225,0],[353,0,226,0],[354,0,227,0],[355,0,228,0],[356,0,229,0],[357,0,230,0],[358,0,231,0],[359,0,232,0],[360,0,233,0],[361,2,234,7],[361,11,234,16,"conv1d"],[361,17,234,22,"conv1d"],[361,18,234,23,"args"],[361,22,234,27],[361,24,234,29],[362,4,235,4],[362,11,235,11],[362,15,235,15,"Conv1D"],[362,35,235,21],[362,36,235,21,"Conv1D"],[362,42,235,21],[362,43,235,22,"args"],[362,47,235,26],[362,48,235,27],[363,2,236,0],[364,2,237,0],[365,0,238,0],[366,0,239,0],[367,0,240,0],[368,0,241,0],[369,0,242,0],[370,0,243,0],[371,0,244,0],[372,0,245,0],[373,0,246,0],[374,0,247,0],[375,0,248,0],[376,0,249,0],[377,0,250,0],[378,0,251,0],[379,0,252,0],[380,0,253,0],[381,0,254,0],[382,2,255,7],[382,11,255,16,"conv2d"],[382,17,255,22,"conv2d"],[382,18,255,23,"args"],[382,22,255,27],[382,24,255,29],[383,4,256,4],[383,11,256,11],[383,15,256,15,"Conv2D"],[383,35,256,21],[383,36,256,21,"Conv2D"],[383,42,256,21],[383,43,256,22,"args"],[383,47,256,26],[383,48,256,27],[384,2,257,0],[385,2,258,0],[386,0,259,0],[387,0,260,0],[388,0,261,0],[389,0,262,0],[390,0,263,0],[391,0,264,0],[392,0,265,0],[393,0,266,0],[394,0,267,0],[395,0,268,0],[396,0,269,0],[397,0,270,0],[398,0,271,0],[399,0,272,0],[400,0,273,0],[401,0,274,0],[402,0,275,0],[403,0,276,0],[404,0,277,0],[405,0,278,0],[406,0,279,0],[407,0,280,0],[408,0,281,0],[409,0,282,0],[410,0,283,0],[411,0,284,0],[412,0,285,0],[413,0,286,0],[414,0,287,0],[415,0,288,0],[416,0,289,0],[417,0,290,0],[418,0,291,0],[419,0,292,0],[420,2,293,7],[420,11,293,16,"conv2dTranspose"],[420,26,293,31,"conv2dTranspose"],[420,27,293,32,"args"],[420,31,293,36],[420,33,293,38],[421,4,294,4],[421,11,294,11],[421,15,294,15,"Conv2DTranspose"],[421,35,294,30],[421,36,294,30,"Conv2DTranspose"],[421,51,294,30],[421,52,294,31,"args"],[421,56,294,35],[421,57,294,36],[422,2,295,0],[423,2,296,0],[424,0,297,0],[425,0,298,0],[426,0,299,0],[427,0,300,0],[428,0,301,0],[429,0,302,0],[430,0,303,0],[431,0,304,0],[432,0,305,0],[433,0,306,0],[434,0,307,0],[435,0,308,0],[436,0,309,0],[437,0,310,0],[438,0,311,0],[439,0,312,0],[440,0,313,0],[441,2,314,7],[441,11,314,16,"conv3d"],[441,17,314,22,"conv3d"],[441,18,314,23,"args"],[441,22,314,27],[441,24,314,29],[442,4,315,4],[442,11,315,11],[442,15,315,15,"Conv3D"],[442,35,315,21],[442,36,315,21,"Conv3D"],[442,42,315,21],[442,43,315,22,"args"],[442,47,315,26],[442,48,315,27],[443,2,316,0],[444,2,317,7],[444,11,317,16,"conv3dTranspose"],[444,26,317,31,"conv3dTranspose"],[444,27,317,32,"args"],[444,31,317,36],[444,33,317,38],[445,4,318,4],[445,11,318,11],[445,15,318,15,"Conv3DTranspose"],[445,35,318,30],[445,36,318,30,"Conv3DTranspose"],[445,51,318,30],[445,52,318,31,"args"],[445,56,318,35],[445,57,318,36],[446,2,319,0],[447,2,320,0],[448,0,321,0],[449,0,322,0],[450,0,323,0],[451,0,324,0],[452,0,325,0],[453,0,326,0],[454,0,327,0],[455,0,328,0],[456,0,329,0],[457,0,330,0],[458,0,331,0],[459,0,332,0],[460,0,333,0],[461,0,334,0],[462,0,335,0],[463,0,336,0],[464,0,337,0],[465,0,338,0],[466,0,339,0],[467,0,340,0],[468,0,341,0],[469,0,342,0],[470,0,343,0],[471,0,344,0],[472,0,345,0],[473,0,346,0],[474,0,347,0],[475,0,348,0],[476,2,349,7],[476,11,349,16,"separableConv2d"],[476,26,349,31,"separableConv2d"],[476,27,349,32,"args"],[476,31,349,36],[476,33,349,38],[477,4,350,4],[477,11,350,11],[477,15,350,15,"SeparableConv2D"],[477,35,350,30],[477,36,350,30,"SeparableConv2D"],[477,51,350,30],[477,52,350,31,"args"],[477,56,350,35],[477,57,350,36],[478,2,351,0],[479,2,352,0],[480,0,353,0],[481,0,354,0],[482,0,355,0],[483,0,356,0],[484,0,357,0],[485,0,358,0],[486,0,359,0],[487,0,360,0],[488,0,361,0],[489,0,362,0],[490,0,363,0],[491,0,364,0],[492,0,365,0],[493,0,366,0],[494,0,367,0],[495,0,368,0],[496,0,369,0],[497,0,370,0],[498,0,371,0],[499,0,372,0],[500,0,373,0],[501,0,374,0],[502,0,375,0],[503,0,376,0],[504,0,377,0],[505,0,378,0],[506,0,379,0],[507,0,380,0],[508,0,381,0],[509,0,382,0],[510,2,383,7],[510,11,383,16,"cropping2D"],[510,21,383,26,"cropping2D"],[510,22,383,27,"args"],[510,26,383,31],[510,28,383,33],[511,4,384,4],[511,11,384,11],[511,15,384,15,"Cropping2D"],[511,35,384,25],[511,36,384,25,"Cropping2D"],[511,46,384,25],[511,47,384,26,"args"],[511,51,384,30],[511,52,384,31],[512,2,385,0],[513,2,386,0],[514,0,387,0],[515,0,388,0],[516,0,389,0],[517,0,390,0],[518,0,391,0],[519,0,392,0],[520,0,393,0],[521,0,394,0],[522,0,395,0],[523,0,396,0],[524,0,397,0],[525,0,398,0],[526,0,399,0],[527,0,400,0],[528,0,401,0],[529,0,402,0],[530,0,403,0],[531,0,404,0],[532,0,405,0],[533,0,406,0],[534,0,407,0],[535,0,408,0],[536,0,409,0],[537,2,410,7],[537,11,410,16,"upSampling2d"],[537,23,410,28,"upSampling2d"],[537,24,410,29,"args"],[537,28,410,33],[537,30,410,35],[538,4,411,4],[538,11,411,11],[538,15,411,15,"UpSampling2D"],[538,35,411,27],[538,36,411,27,"UpSampling2D"],[538,48,411,27],[538,49,411,28,"args"],[538,53,411,32],[538,54,411,33],[539,2,412,0],[540,2,413,0],[541,2,414,0],[542,0,415,0],[543,0,416,0],[544,0,417,0],[545,0,418,0],[546,0,419,0],[547,0,420,0],[548,0,421,0],[549,0,422,0],[550,0,423,0],[551,2,424,7],[551,11,424,16,"depthwiseConv2d"],[551,26,424,31,"depthwiseConv2d"],[551,27,424,32,"args"],[551,31,424,36],[551,33,424,38],[552,4,425,4],[552,11,425,11],[552,15,425,15,"DepthwiseConv2D"],[552,45,425,30],[552,46,425,30,"DepthwiseConv2D"],[552,61,425,30],[552,62,425,31,"args"],[552,66,425,35],[552,67,425,36],[553,2,426,0],[554,2,427,0],[555,2,428,0],[556,0,429,0],[557,0,430,0],[558,0,431,0],[559,0,432,0],[560,0,433,0],[561,0,434,0],[562,0,435,0],[563,0,436,0],[564,0,437,0],[565,0,438,0],[566,0,439,0],[567,0,440,0],[568,0,441,0],[569,0,442,0],[570,0,443,0],[571,0,444,0],[572,0,445,0],[573,0,446,0],[574,0,447,0],[575,0,448,0],[576,0,449,0],[577,0,450,0],[578,0,451,0],[579,0,452,0],[580,0,453,0],[581,0,454,0],[582,0,455,0],[583,0,456,0],[584,0,457,0],[585,0,458,0],[586,2,459,7],[586,11,459,16,"activation"],[586,21,459,26,"activation"],[586,22,459,27,"args"],[586,26,459,31],[586,28,459,33],[587,4,460,4],[587,11,460,11],[587,15,460,15,"Activation"],[587,26,460,25],[587,27,460,25,"Activation"],[587,37,460,25],[587,38,460,26,"args"],[587,42,460,30],[587,43,460,31],[588,2,461,0],[589,2,462,0],[590,0,463,0],[591,0,464,0],[592,0,465,0],[593,0,466,0],[594,0,467,0],[595,0,468,0],[596,0,469,0],[597,0,470,0],[598,0,471,0],[599,0,472,0],[600,0,473,0],[601,0,474,0],[602,0,475,0],[603,0,476,0],[604,0,477,0],[605,0,478,0],[606,0,479,0],[607,0,480,0],[608,0,481,0],[609,0,482,0],[610,0,483,0],[611,0,484,0],[612,0,485,0],[613,0,486,0],[614,0,487,0],[615,0,488,0],[616,0,489,0],[617,0,490,0],[618,0,491,0],[619,0,492,0],[620,0,493,0],[621,0,494,0],[622,2,495,7],[622,11,495,16,"dense"],[622,16,495,21,"dense"],[622,17,495,22,"args"],[622,21,495,26],[622,23,495,28],[623,4,496,4],[623,11,496,11],[623,15,496,15,"Dense"],[623,26,496,20],[623,27,496,20,"Dense"],[623,32,496,20],[623,33,496,21,"args"],[623,37,496,25],[623,38,496,26],[624,2,497,0],[625,2,498,0],[626,0,499,0],[627,0,500,0],[628,0,501,0],[629,0,502,0],[630,0,503,0],[631,0,504,0],[632,0,505,0],[633,0,506,0],[634,0,507,0],[635,2,508,7],[635,11,508,16,"dropout"],[635,18,508,23,"dropout"],[635,19,508,24,"args"],[635,23,508,28],[635,25,508,30],[636,4,509,4],[636,11,509,11],[636,15,509,15,"Dropout"],[636,26,509,22],[636,27,509,22,"Dropout"],[636,34,509,22],[636,35,509,23,"args"],[636,39,509,27],[636,40,509,28],[637,2,510,0],[638,2,511,0],[639,0,512,0],[640,0,513,0],[641,0,514,0],[642,0,515,0],[643,0,516,0],[644,0,517,0],[645,0,518,0],[646,0,519,0],[647,0,520,0],[648,0,521,0],[649,0,522,0],[650,0,523,0],[651,0,524,0],[652,0,525,0],[653,0,526,0],[654,0,527,0],[655,0,528,0],[656,0,529,0],[657,0,530,0],[658,0,531,0],[659,0,532,0],[660,0,533,0],[661,0,534,0],[662,0,535,0],[663,0,536,0],[664,0,537,0],[665,0,538,0],[666,0,539,0],[667,0,540,0],[668,0,541,0],[669,0,542,0],[670,2,543,7],[670,11,543,16,"spatialDropout1d"],[670,27,543,32,"spatialDropout1d"],[670,28,543,33,"args"],[670,32,543,37],[670,34,543,39],[671,4,544,4],[671,11,544,11],[671,15,544,15,"SpatialDropout1D"],[671,26,544,31],[671,27,544,31,"SpatialDropout1D"],[671,43,544,31],[671,44,544,32,"args"],[671,48,544,36],[671,49,544,37],[672,2,545,0],[673,2,546,0],[674,0,547,0],[675,0,548,0],[676,0,549,0],[677,0,550,0],[678,0,551,0],[679,0,552,0],[680,0,553,0],[681,0,554,0],[682,0,555,0],[683,0,556,0],[684,0,557,0],[685,0,558,0],[686,0,559,0],[687,0,560,0],[688,0,561,0],[689,0,562,0],[690,0,563,0],[691,0,564,0],[692,2,565,7],[692,11,565,16,"flatten"],[692,18,565,23,"flatten"],[692,19,565,24,"args"],[692,23,565,28],[692,25,565,30],[693,4,566,4],[693,11,566,11],[693,15,566,15,"Flatten"],[693,26,566,22],[693,27,566,22,"Flatten"],[693,34,566,22],[693,35,566,23,"args"],[693,39,566,27],[693,40,566,28],[694,2,567,0],[695,2,568,0],[696,0,569,0],[697,0,570,0],[698,0,571,0],[699,0,572,0],[700,0,573,0],[701,0,574,0],[702,0,575,0],[703,0,576,0],[704,0,577,0],[705,0,578,0],[706,0,579,0],[707,0,580,0],[708,0,581,0],[709,2,582,7],[709,11,582,16,"repeatVector"],[709,23,582,28,"repeatVector"],[709,24,582,29,"args"],[709,28,582,33],[709,30,582,35],[710,4,583,4],[710,11,583,11],[710,15,583,15,"RepeatVector"],[710,26,583,27],[710,27,583,27,"RepeatVector"],[710,39,583,27],[710,40,583,28,"args"],[710,44,583,32],[710,45,583,33],[711,2,584,0],[712,2,585,0],[713,0,586,0],[714,0,587,0],[715,0,588,0],[716,0,589,0],[717,0,590,0],[718,0,591,0],[719,0,592,0],[720,0,593,0],[721,0,594,0],[722,0,595,0],[723,0,596,0],[724,0,597,0],[725,0,598,0],[726,0,599,0],[727,0,600,0],[728,0,601,0],[729,0,602,0],[730,0,603,0],[731,0,604,0],[732,0,605,0],[733,0,606,0],[734,0,607,0],[735,2,608,7],[735,11,608,16,"reshape"],[735,18,608,23,"reshape"],[735,19,608,24,"args"],[735,23,608,28],[735,25,608,30],[736,4,609,4],[736,11,609,11],[736,15,609,15,"Reshape"],[736,26,609,22],[736,27,609,22,"Reshape"],[736,34,609,22],[736,35,609,23,"args"],[736,39,609,27],[736,40,609,28],[737,2,610,0],[738,2,611,0],[739,0,612,0],[740,0,613,0],[741,0,614,0],[742,0,615,0],[743,0,616,0],[744,0,617,0],[745,0,618,0],[746,0,619,0],[747,0,620,0],[748,0,621,0],[749,0,622,0],[750,0,623,0],[751,0,624,0],[752,0,625,0],[753,0,626,0],[754,0,627,0],[755,0,628,0],[756,0,629,0],[757,0,630,0],[758,0,631,0],[759,0,632,0],[760,0,633,0],[761,0,634,0],[762,0,635,0],[763,0,636,0],[764,0,637,0],[765,0,638,0],[766,2,639,7],[766,11,639,16,"permute"],[766,18,639,23,"permute"],[766,19,639,24,"args"],[766,23,639,28],[766,25,639,30],[767,4,640,4],[767,11,640,11],[767,15,640,15,"Permute"],[767,26,640,22],[767,27,640,22,"Permute"],[767,34,640,22],[767,35,640,23,"args"],[767,39,640,27],[767,40,640,28],[768,2,641,0],[769,2,642,0],[770,0,643,0],[771,0,644,0],[772,0,645,0],[773,0,646,0],[774,0,647,0],[775,0,648,0],[776,0,649,0],[777,0,650,0],[778,0,651,0],[779,0,652,0],[780,2,653,7],[780,11,653,16,"embedding"],[780,20,653,25,"embedding"],[780,21,653,26,"args"],[780,25,653,30],[780,27,653,32],[781,4,654,4],[781,11,654,11],[781,15,654,15,"Embedding"],[781,32,654,24],[781,33,654,24,"Embedding"],[781,42,654,24],[781,43,654,25,"args"],[781,47,654,29],[781,48,654,30],[782,2,655,0],[783,2,656,0],[784,2,657,0],[785,0,658,0],[786,0,659,0],[787,0,660,0],[788,0,661,0],[789,0,662,0],[790,0,663,0],[791,0,664,0],[792,0,665,0],[793,0,666,0],[794,0,667,0],[795,0,668,0],[796,0,669,0],[797,0,670,0],[798,0,671,0],[799,0,672,0],[800,0,673,0],[801,0,674,0],[802,0,675,0],[803,0,676,0],[804,2,677,7],[804,11,677,16,"add"],[804,14,677,19,"add"],[804,15,677,20,"args"],[804,19,677,24],[804,21,677,26],[805,4,678,4],[805,11,678,11],[805,15,678,15,"Add"],[805,27,678,18],[805,28,678,18,"Add"],[805,31,678,18],[805,32,678,19,"args"],[805,36,678,23],[805,37,678,24],[806,2,679,0],[807,2,680,0],[808,0,681,0],[809,0,682,0],[810,0,683,0],[811,0,684,0],[812,0,685,0],[813,0,686,0],[814,0,687,0],[815,0,688,0],[816,0,689,0],[817,0,690,0],[818,0,691,0],[819,0,692,0],[820,0,693,0],[821,0,694,0],[822,0,695,0],[823,0,696,0],[824,0,697,0],[825,2,698,7],[825,11,698,16,"average"],[825,18,698,23,"average"],[825,19,698,24,"args"],[825,23,698,28],[825,25,698,30],[826,4,699,4],[826,11,699,11],[826,15,699,15,"Average"],[826,27,699,22],[826,28,699,22,"Average"],[826,35,699,22],[826,36,699,23,"args"],[826,40,699,27],[826,41,699,28],[827,2,700,0],[828,2,701,0],[829,0,702,0],[830,0,703,0],[831,0,704,0],[832,0,705,0],[833,0,706,0],[834,0,707,0],[835,0,708,0],[836,0,709,0],[837,0,710,0],[838,0,711,0],[839,0,712,0],[840,0,713,0],[841,0,714,0],[842,0,715,0],[843,0,716,0],[844,0,717,0],[845,0,718,0],[846,0,719,0],[847,0,720,0],[848,2,721,7],[848,11,721,16,"concatenate"],[848,22,721,27,"concatenate"],[848,23,721,28,"args"],[848,27,721,32],[848,29,721,34],[849,4,722,4],[849,11,722,11],[849,15,722,15,"Concatenate"],[849,27,722,26],[849,28,722,26,"Concatenate"],[849,39,722,26],[849,40,722,27,"args"],[849,44,722,31],[849,45,722,32],[850,2,723,0],[851,2,724,0],[852,0,725,0],[853,0,726,0],[854,0,727,0],[855,0,728,0],[856,0,729,0],[857,0,730,0],[858,0,731,0],[859,0,732,0],[860,0,733,0],[861,0,734,0],[862,0,735,0],[863,0,736,0],[864,0,737,0],[865,0,738,0],[866,0,739,0],[867,0,740,0],[868,0,741,0],[869,2,742,7],[869,11,742,16,"maximum"],[869,18,742,23,"maximum"],[869,19,742,24,"args"],[869,23,742,28],[869,25,742,30],[870,4,743,4],[870,11,743,11],[870,15,743,15,"Maximum"],[870,27,743,22],[870,28,743,22,"Maximum"],[870,35,743,22],[870,36,743,23,"args"],[870,40,743,27],[870,41,743,28],[871,2,744,0],[872,2,745,0],[873,0,746,0],[874,0,747,0],[875,0,748,0],[876,0,749,0],[877,0,750,0],[878,0,751,0],[879,0,752,0],[880,0,753,0],[881,0,754,0],[882,0,755,0],[883,0,756,0],[884,0,757,0],[885,0,758,0],[886,0,759,0],[887,0,760,0],[888,0,761,0],[889,0,762,0],[890,2,763,7],[890,11,763,16,"minimum"],[890,18,763,23,"minimum"],[890,19,763,24,"args"],[890,23,763,28],[890,25,763,30],[891,4,764,4],[891,11,764,11],[891,15,764,15,"Minimum"],[891,27,764,22],[891,28,764,22,"Minimum"],[891,35,764,22],[891,36,764,23,"args"],[891,40,764,27],[891,41,764,28],[892,2,765,0],[893,2,766,0],[894,0,767,0],[895,0,768,0],[896,0,769,0],[897,0,770,0],[898,0,771,0],[899,0,772,0],[900,0,773,0],[901,0,774,0],[902,0,775,0],[903,0,776,0],[904,0,777,0],[905,0,778,0],[906,0,779,0],[907,0,780,0],[908,0,781,0],[909,0,782,0],[910,0,783,0],[911,0,784,0],[912,2,785,7],[912,11,785,16,"multiply"],[912,19,785,24,"multiply"],[912,20,785,25,"args"],[912,24,785,29],[912,26,785,31],[913,4,786,4],[913,11,786,11],[913,15,786,15,"Multiply"],[913,27,786,23],[913,28,786,23,"Multiply"],[913,36,786,23],[913,37,786,24,"args"],[913,41,786,28],[913,42,786,29],[914,2,787,0],[915,2,788,0],[916,0,789,0],[917,0,790,0],[918,0,791,0],[919,0,792,0],[920,0,793,0],[921,0,794,0],[922,0,795,0],[923,0,796,0],[924,0,797,0],[925,0,798,0],[926,0,799,0],[927,0,800,0],[928,0,801,0],[929,0,802,0],[930,0,803,0],[931,0,804,0],[932,0,805,0],[933,0,806,0],[934,0,807,0],[935,0,808,0],[936,0,809,0],[937,2,810,7],[937,11,810,16,"dot"],[937,14,810,19,"dot"],[937,15,810,20,"args"],[937,19,810,24],[937,21,810,26],[938,4,811,4],[938,11,811,11],[938,15,811,15,"Dot"],[938,27,811,18],[938,28,811,18,"Dot"],[938,31,811,18],[938,32,811,19,"args"],[938,36,811,23],[938,37,811,24],[939,2,812,0],[940,2,813,0],[941,2,814,0],[942,0,815,0],[943,0,816,0],[944,0,817,0],[945,0,818,0],[946,0,819,0],[947,0,820,0],[948,0,821,0],[949,0,822,0],[950,0,823,0],[951,0,824,0],[952,0,825,0],[953,0,826,0],[954,0,827,0],[955,0,828,0],[956,0,829,0],[957,0,830,0],[958,0,831,0],[959,0,832,0],[960,0,833,0],[961,0,834,0],[962,2,835,7],[962,11,835,16,"batchNormalization"],[962,29,835,34,"batchNormalization"],[962,30,835,35,"args"],[962,34,835,39],[962,36,835,41],[963,4,836,4],[963,11,836,11],[963,15,836,15,"BatchNormalization"],[963,35,836,33],[963,36,836,33,"BatchNormalization"],[963,54,836,33],[963,55,836,34,"args"],[963,59,836,38],[963,60,836,39],[964,2,837,0],[965,2,838,0],[966,0,839,0],[967,0,840,0],[968,0,841,0],[969,0,842,0],[970,0,843,0],[971,0,844,0],[972,0,845,0],[973,0,846,0],[974,0,847,0],[975,0,848,0],[976,0,849,0],[977,0,850,0],[978,0,851,0],[979,0,852,0],[980,0,853,0],[981,0,854,0],[982,0,855,0],[983,0,856,0],[984,0,857,0],[985,2,858,7],[985,11,858,16,"layerNormalization"],[985,29,858,34,"layerNormalization"],[985,30,858,35,"args"],[985,34,858,39],[985,36,858,41],[986,4,859,4],[986,11,859,11],[986,15,859,15,"LayerNormalization"],[986,35,859,33],[986,36,859,33,"LayerNormalization"],[986,54,859,33],[986,55,859,34,"args"],[986,59,859,38],[986,60,859,39],[987,2,860,0],[988,2,861,0],[989,2,862,0],[990,0,863,0],[991,0,864,0],[992,0,865,0],[993,0,866,0],[994,0,867,0],[995,0,868,0],[996,0,869,0],[997,0,870,0],[998,0,871,0],[999,0,872,0],[1000,0,873,0],[1001,0,874,0],[1002,0,875,0],[1003,0,876,0],[1004,0,877,0],[1005,0,878,0],[1006,0,879,0],[1007,0,880,0],[1008,0,881,0],[1009,0,882,0],[1010,0,883,0],[1011,2,884,7],[1011,11,884,16,"zeroPadding2d"],[1011,24,884,29,"zeroPadding2d"],[1011,25,884,30,"args"],[1011,29,884,34],[1011,31,884,36],[1012,4,885,4],[1012,11,885,11],[1012,15,885,15,"ZeroPadding2D"],[1012,29,885,28],[1012,30,885,28,"ZeroPadding2D"],[1012,43,885,28],[1012,44,885,29,"args"],[1012,48,885,33],[1012,49,885,34],[1013,2,886,0],[1014,2,887,0],[1015,2,888,0],[1016,0,889,0],[1017,0,890,0],[1018,0,891,0],[1019,0,892,0],[1020,0,893,0],[1021,0,894,0],[1022,0,895,0],[1023,0,896,0],[1024,0,897,0],[1025,0,898,0],[1026,2,899,7],[1026,11,899,16,"averagePooling1d"],[1026,27,899,32,"averagePooling1d"],[1026,28,899,33,"args"],[1026,32,899,37],[1026,34,899,39],[1027,4,900,4],[1027,11,900,11],[1027,15,900,15,"AveragePooling1D"],[1027,29,900,31],[1027,30,900,31,"AveragePooling1D"],[1027,46,900,31],[1027,47,900,32,"args"],[1027,51,900,36],[1027,52,900,37],[1028,2,901,0],[1029,2,902,7],[1029,11,902,16,"avgPool1d"],[1029,20,902,25,"avgPool1d"],[1029,21,902,26,"args"],[1029,25,902,30],[1029,27,902,32],[1030,4,903,4],[1030,11,903,11,"averagePooling1d"],[1030,27,903,27],[1030,28,903,28,"args"],[1030,32,903,32],[1030,33,903,33],[1031,2,904,0],[1032,2,905,0],[1033,2,906,0],[1034,2,907,7],[1034,11,907,16,"avgPooling1d"],[1034,23,907,28,"avgPooling1d"],[1034,24,907,29,"args"],[1034,28,907,33],[1034,30,907,35],[1035,4,908,4],[1035,11,908,11,"averagePooling1d"],[1035,27,908,27],[1035,28,908,28,"args"],[1035,32,908,32],[1035,33,908,33],[1036,2,909,0],[1037,2,910,0],[1038,0,911,0],[1039,0,912,0],[1040,0,913,0],[1041,0,914,0],[1042,0,915,0],[1043,0,916,0],[1044,0,917,0],[1045,0,918,0],[1046,0,919,0],[1047,0,920,0],[1048,0,921,0],[1049,0,922,0],[1050,0,923,0],[1051,0,924,0],[1052,0,925,0],[1053,0,926,0],[1054,0,927,0],[1055,0,928,0],[1056,0,929,0],[1057,0,930,0],[1058,0,931,0],[1059,0,932,0],[1060,2,933,7],[1060,11,933,16,"averagePooling2d"],[1060,27,933,32,"averagePooling2d"],[1060,28,933,33,"args"],[1060,32,933,37],[1060,34,933,39],[1061,4,934,4],[1061,11,934,11],[1061,15,934,15,"AveragePooling2D"],[1061,29,934,31],[1061,30,934,31,"AveragePooling2D"],[1061,46,934,31],[1061,47,934,32,"args"],[1061,51,934,36],[1061,52,934,37],[1062,2,935,0],[1063,2,936,7],[1063,11,936,16,"avgPool2d"],[1063,20,936,25,"avgPool2d"],[1063,21,936,26,"args"],[1063,25,936,30],[1063,27,936,32],[1064,4,937,4],[1064,11,937,11,"averagePooling2d"],[1064,27,937,27],[1064,28,937,28,"args"],[1064,32,937,32],[1064,33,937,33],[1065,2,938,0],[1066,2,939,0],[1067,2,940,0],[1068,2,941,7],[1068,11,941,16,"avgPooling2d"],[1068,23,941,28,"avgPooling2d"],[1068,24,941,29,"args"],[1068,28,941,33],[1068,30,941,35],[1069,4,942,4],[1069,11,942,11,"averagePooling2d"],[1069,27,942,27],[1069,28,942,28,"args"],[1069,32,942,32],[1069,33,942,33],[1070,2,943,0],[1071,2,944,0],[1072,0,945,0],[1073,0,946,0],[1074,0,947,0],[1075,0,948,0],[1076,0,949,0],[1077,0,950,0],[1078,0,951,0],[1079,0,952,0],[1080,0,953,0],[1081,0,954,0],[1082,0,955,0],[1083,0,956,0],[1084,0,957,0],[1085,0,958,0],[1086,0,959,0],[1087,0,960,0],[1088,0,961,0],[1089,0,962,0],[1090,0,963,0],[1091,0,964,0],[1092,2,965,7],[1092,11,965,16,"averagePooling3d"],[1092,27,965,32,"averagePooling3d"],[1092,28,965,33,"args"],[1092,32,965,37],[1092,34,965,39],[1093,4,966,4],[1093,11,966,11],[1093,15,966,15,"AveragePooling3D"],[1093,29,966,31],[1093,30,966,31,"AveragePooling3D"],[1093,46,966,31],[1093,47,966,32,"args"],[1093,51,966,36],[1093,52,966,37],[1094,2,967,0],[1095,2,968,7],[1095,11,968,16,"avgPool3d"],[1095,20,968,25,"avgPool3d"],[1095,21,968,26,"args"],[1095,25,968,30],[1095,27,968,32],[1096,4,969,4],[1096,11,969,11,"averagePooling3d"],[1096,27,969,27],[1096,28,969,28,"args"],[1096,32,969,32],[1096,33,969,33],[1097,2,970,0],[1098,2,971,0],[1099,2,972,0],[1100,2,973,7],[1100,11,973,16,"avgPooling3d"],[1100,23,973,28,"avgPooling3d"],[1100,24,973,29,"args"],[1100,28,973,33],[1100,30,973,35],[1101,4,974,4],[1101,11,974,11,"averagePooling3d"],[1101,27,974,27],[1101,28,974,28,"args"],[1101,32,974,32],[1101,33,974,33],[1102,2,975,0],[1103,2,976,0],[1104,0,977,0],[1105,0,978,0],[1106,0,979,0],[1107,0,980,0],[1108,0,981,0],[1109,0,982,0],[1110,0,983,0],[1111,0,984,0],[1112,2,985,7],[1112,11,985,16,"globalAveragePooling1d"],[1112,33,985,38,"globalAveragePooling1d"],[1112,34,985,39,"args"],[1112,38,985,43],[1112,40,985,45],[1113,4,986,4],[1113,11,986,11],[1113,15,986,15,"GlobalAveragePooling1D"],[1113,29,986,37],[1113,30,986,37,"GlobalAveragePooling1D"],[1113,52,986,37],[1113,53,986,38,"args"],[1113,57,986,42],[1113,58,986,43],[1114,2,987,0],[1115,2,988,0],[1116,0,989,0],[1117,0,990,0],[1118,0,991,0],[1119,0,992,0],[1120,0,993,0],[1121,0,994,0],[1122,0,995,0],[1123,0,996,0],[1124,0,997,0],[1125,0,998,0],[1126,0,999,0],[1127,0,1000,0],[1128,0,1001,0],[1129,2,1002,7],[1129,11,1002,16,"globalAveragePooling2d"],[1129,33,1002,38,"globalAveragePooling2d"],[1129,34,1002,39,"args"],[1129,38,1002,43],[1129,40,1002,45],[1130,4,1003,4],[1130,11,1003,11],[1130,15,1003,15,"GlobalAveragePooling2D"],[1130,29,1003,37],[1130,30,1003,37,"GlobalAveragePooling2D"],[1130,52,1003,37],[1130,53,1003,38,"args"],[1130,57,1003,42],[1130,58,1003,43],[1131,2,1004,0],[1132,2,1005,0],[1133,0,1006,0],[1134,0,1007,0],[1135,0,1008,0],[1136,0,1009,0],[1137,0,1010,0],[1138,0,1011,0],[1139,0,1012,0],[1140,0,1013,0],[1141,2,1014,7],[1141,11,1014,16,"globalMaxPooling1d"],[1141,29,1014,34,"globalMaxPooling1d"],[1141,30,1014,35,"args"],[1141,34,1014,39],[1141,36,1014,41],[1142,4,1015,4],[1142,11,1015,11],[1142,15,1015,15,"GlobalMaxPooling1D"],[1142,29,1015,33],[1142,30,1015,33,"GlobalMaxPooling1D"],[1142,48,1015,33],[1142,49,1015,34,"args"],[1142,53,1015,38],[1142,54,1015,39],[1143,2,1016,0],[1144,2,1017,0],[1145,0,1018,0],[1146,0,1019,0],[1147,0,1020,0],[1148,0,1021,0],[1149,0,1022,0],[1150,0,1023,0],[1151,0,1024,0],[1152,0,1025,0],[1153,0,1026,0],[1154,0,1027,0],[1155,0,1028,0],[1156,0,1029,0],[1157,0,1030,0],[1158,2,1031,7],[1158,11,1031,16,"globalMaxPooling2d"],[1158,29,1031,34,"globalMaxPooling2d"],[1158,30,1031,35,"args"],[1158,34,1031,39],[1158,36,1031,41],[1159,4,1032,4],[1159,11,1032,11],[1159,15,1032,15,"GlobalMaxPooling2D"],[1159,29,1032,33],[1159,30,1032,33,"GlobalMaxPooling2D"],[1159,48,1032,33],[1159,49,1032,34,"args"],[1159,53,1032,38],[1159,54,1032,39],[1160,2,1033,0],[1161,2,1034,0],[1162,0,1035,0],[1163,0,1036,0],[1164,0,1037,0],[1165,0,1038,0],[1166,0,1039,0],[1167,0,1040,0],[1168,0,1041,0],[1169,0,1042,0],[1170,2,1043,7],[1170,11,1043,16,"maxPooling1d"],[1170,23,1043,28,"maxPooling1d"],[1170,24,1043,29,"args"],[1170,28,1043,33],[1170,30,1043,35],[1171,4,1044,4],[1171,11,1044,11],[1171,15,1044,15,"MaxPooling1D"],[1171,29,1044,27],[1171,30,1044,27,"MaxPooling1D"],[1171,42,1044,27],[1171,43,1044,28,"args"],[1171,47,1044,32],[1171,48,1044,33],[1172,2,1045,0],[1173,2,1046,0],[1174,0,1047,0],[1175,0,1048,0],[1176,0,1049,0],[1177,0,1050,0],[1178,0,1051,0],[1179,0,1052,0],[1180,0,1053,0],[1181,0,1054,0],[1182,0,1055,0],[1183,0,1056,0],[1184,0,1057,0],[1185,0,1058,0],[1186,0,1059,0],[1187,0,1060,0],[1188,0,1061,0],[1189,0,1062,0],[1190,0,1063,0],[1191,0,1064,0],[1192,0,1065,0],[1193,0,1066,0],[1194,2,1067,7],[1194,11,1067,16,"maxPooling2d"],[1194,23,1067,28,"maxPooling2d"],[1194,24,1067,29,"args"],[1194,28,1067,33],[1194,30,1067,35],[1195,4,1068,4],[1195,11,1068,11],[1195,15,1068,15,"MaxPooling2D"],[1195,29,1068,27],[1195,30,1068,27,"MaxPooling2D"],[1195,42,1068,27],[1195,43,1068,28,"args"],[1195,47,1068,32],[1195,48,1068,33],[1196,2,1069,0],[1197,2,1070,0],[1198,0,1071,0],[1199,0,1072,0],[1200,0,1073,0],[1201,0,1074,0],[1202,0,1075,0],[1203,0,1076,0],[1204,0,1077,0],[1205,0,1078,0],[1206,0,1079,0],[1207,0,1080,0],[1208,0,1081,0],[1209,0,1082,0],[1210,0,1083,0],[1211,0,1084,0],[1212,0,1085,0],[1213,0,1086,0],[1214,0,1087,0],[1215,0,1088,0],[1216,0,1089,0],[1217,0,1090,0],[1218,2,1091,7],[1218,11,1091,16,"maxPooling3d"],[1218,23,1091,28,"maxPooling3d"],[1218,24,1091,29,"args"],[1218,28,1091,33],[1218,30,1091,35],[1219,4,1092,4],[1219,11,1092,11],[1219,15,1092,15,"MaxPooling3D"],[1219,29,1092,27],[1219,30,1092,27,"MaxPooling3D"],[1219,42,1092,27],[1219,43,1092,28,"args"],[1219,47,1092,32],[1219,48,1092,33],[1220,2,1093,0],[1221,2,1094,0],[1222,2,1095,0],[1223,0,1096,0],[1224,0,1097,0],[1225,0,1098,0],[1226,0,1099,0],[1227,0,1100,0],[1228,0,1101,0],[1229,0,1102,0],[1230,0,1103,0],[1231,0,1104,0],[1232,0,1105,0],[1233,0,1106,0],[1234,0,1107,0],[1235,0,1108,0],[1236,0,1109,0],[1237,0,1110,0],[1238,0,1111,0],[1239,0,1112,0],[1240,0,1113,0],[1241,0,1114,0],[1242,0,1115,0],[1243,0,1116,0],[1244,0,1117,0],[1245,2,1118,7],[1245,11,1118,16,"gru"],[1245,14,1118,19,"gru"],[1245,15,1118,20,"args"],[1245,19,1118,24],[1245,21,1118,26],[1246,4,1119,4],[1246,11,1119,11],[1246,15,1119,15,"GRU"],[1246,31,1119,18],[1246,32,1119,18,"GRU"],[1246,35,1119,18],[1246,36,1119,19,"args"],[1246,40,1119,23],[1246,41,1119,24],[1247,2,1120,0],[1248,2,1121,0],[1249,0,1122,0],[1250,0,1123,0],[1251,0,1124,0],[1252,0,1125,0],[1253,0,1126,0],[1254,0,1127,0],[1255,0,1128,0],[1256,0,1129,0],[1257,0,1130,0],[1258,0,1131,0],[1259,0,1132,0],[1260,0,1133,0],[1261,0,1134,0],[1262,0,1135,0],[1263,0,1136,0],[1264,0,1137,0],[1265,0,1138,0],[1266,0,1139,0],[1267,0,1140,0],[1268,0,1141,0],[1269,0,1142,0],[1270,0,1143,0],[1271,0,1144,0],[1272,0,1145,0],[1273,0,1146,0],[1274,0,1147,0],[1275,0,1148,0],[1276,0,1149,0],[1277,0,1150,0],[1278,0,1151,0],[1279,0,1152,0],[1280,0,1153,0],[1281,0,1154,0],[1282,0,1155,0],[1283,0,1156,0],[1284,0,1157,0],[1285,0,1158,0],[1286,0,1159,0],[1287,0,1160,0],[1288,0,1161,0],[1289,0,1162,0],[1290,0,1163,0],[1291,0,1164,0],[1292,0,1165,0],[1293,2,1166,7],[1293,11,1166,16,"gruCell"],[1293,18,1166,23,"gruCell"],[1293,19,1166,24,"args"],[1293,23,1166,28],[1293,25,1166,30],[1294,4,1167,4],[1294,11,1167,11],[1294,15,1167,15,"GRUCell"],[1294,31,1167,22],[1294,32,1167,22,"GRUCell"],[1294,39,1167,22],[1294,40,1167,23,"args"],[1294,44,1167,27],[1294,45,1167,28],[1295,2,1168,0],[1296,2,1169,0],[1297,0,1170,0],[1298,0,1171,0],[1299,0,1172,0],[1300,0,1173,0],[1301,0,1174,0],[1302,0,1175,0],[1303,0,1176,0],[1304,0,1177,0],[1305,0,1178,0],[1306,0,1179,0],[1307,0,1180,0],[1308,0,1181,0],[1309,0,1182,0],[1310,0,1183,0],[1311,0,1184,0],[1312,0,1185,0],[1313,0,1186,0],[1314,0,1187,0],[1315,0,1188,0],[1316,0,1189,0],[1317,0,1190,0],[1318,0,1191,0],[1319,2,1192,7],[1319,11,1192,16,"lstm"],[1319,15,1192,20,"lstm"],[1319,16,1192,21,"args"],[1319,20,1192,25],[1319,22,1192,27],[1320,4,1193,4],[1320,11,1193,11],[1320,15,1193,15,"LSTM"],[1320,31,1193,19],[1320,32,1193,19,"LSTM"],[1320,36,1193,19],[1320,37,1193,20,"args"],[1320,41,1193,24],[1320,42,1193,25],[1321,2,1194,0],[1322,2,1195,0],[1323,0,1196,0],[1324,0,1197,0],[1325,0,1198,0],[1326,0,1199,0],[1327,0,1200,0],[1328,0,1201,0],[1329,0,1202,0],[1330,0,1203,0],[1331,0,1204,0],[1332,0,1205,0],[1333,0,1206,0],[1334,0,1207,0],[1335,0,1208,0],[1336,0,1209,0],[1337,0,1210,0],[1338,0,1211,0],[1339,0,1212,0],[1340,0,1213,0],[1341,0,1214,0],[1342,0,1215,0],[1343,0,1216,0],[1344,0,1217,0],[1345,0,1218,0],[1346,0,1219,0],[1347,0,1220,0],[1348,0,1221,0],[1349,0,1222,0],[1350,0,1223,0],[1351,0,1224,0],[1352,0,1225,0],[1353,0,1226,0],[1354,0,1227,0],[1355,0,1228,0],[1356,0,1229,0],[1357,0,1230,0],[1358,0,1231,0],[1359,0,1232,0],[1360,0,1233,0],[1361,0,1234,0],[1362,0,1235,0],[1363,0,1236,0],[1364,0,1237,0],[1365,0,1238,0],[1366,0,1239,0],[1367,2,1240,7],[1367,11,1240,16,"lstmCell"],[1367,19,1240,24,"lstmCell"],[1367,20,1240,25,"args"],[1367,24,1240,29],[1367,26,1240,31],[1368,4,1241,4],[1368,11,1241,11],[1368,15,1241,15,"LSTMCell"],[1368,31,1241,23],[1368,32,1241,23,"LSTMCell"],[1368,40,1241,23],[1368,41,1241,24,"args"],[1368,45,1241,28],[1368,46,1241,29],[1369,2,1242,0],[1370,2,1243,0],[1371,0,1244,0],[1372,0,1245,0],[1373,0,1246,0],[1374,0,1247,0],[1375,0,1248,0],[1376,0,1249,0],[1377,0,1250,0],[1378,0,1251,0],[1379,0,1252,0],[1380,0,1253,0],[1381,0,1254,0],[1382,0,1255,0],[1383,0,1256,0],[1384,0,1257,0],[1385,0,1258,0],[1386,0,1259,0],[1387,0,1260,0],[1388,0,1261,0],[1389,0,1262,0],[1390,0,1263,0],[1391,0,1264,0],[1392,0,1265,0],[1393,0,1266,0],[1394,2,1267,7],[1394,11,1267,16,"simpleRNN"],[1394,20,1267,25,"simpleRNN"],[1394,21,1267,26,"args"],[1394,25,1267,30],[1394,27,1267,32],[1395,4,1268,4],[1395,11,1268,11],[1395,15,1268,15,"SimpleRNN"],[1395,31,1268,24],[1395,32,1268,24,"SimpleRNN"],[1395,41,1268,24],[1395,42,1268,25,"args"],[1395,46,1268,29],[1395,47,1268,30],[1396,2,1269,0],[1397,2,1270,0],[1398,0,1271,0],[1399,0,1272,0],[1400,0,1273,0],[1401,0,1274,0],[1402,0,1275,0],[1403,0,1276,0],[1404,0,1277,0],[1405,0,1278,0],[1406,0,1279,0],[1407,0,1280,0],[1408,0,1281,0],[1409,0,1282,0],[1410,0,1283,0],[1411,0,1284,0],[1412,0,1285,0],[1413,0,1286,0],[1414,0,1287,0],[1415,0,1288,0],[1416,0,1289,0],[1417,0,1290,0],[1418,0,1291,0],[1419,0,1292,0],[1420,0,1293,0],[1421,0,1294,0],[1422,0,1295,0],[1423,0,1296,0],[1424,0,1297,0],[1425,0,1298,0],[1426,0,1299,0],[1427,0,1300,0],[1428,0,1301,0],[1429,0,1302,0],[1430,0,1303,0],[1431,0,1304,0],[1432,0,1305,0],[1433,0,1306,0],[1434,0,1307,0],[1435,0,1308,0],[1436,0,1309,0],[1437,0,1310,0],[1438,0,1311,0],[1439,0,1312,0],[1440,0,1313,0],[1441,0,1314,0],[1442,2,1315,7],[1442,11,1315,16,"simpleRNNCell"],[1442,24,1315,29,"simpleRNNCell"],[1442,25,1315,30,"args"],[1442,29,1315,34],[1442,31,1315,36],[1443,4,1316,4],[1443,11,1316,11],[1443,15,1316,15,"SimpleRNNCell"],[1443,31,1316,28],[1443,32,1316,28,"SimpleRNNCell"],[1443,45,1316,28],[1443,46,1316,29,"args"],[1443,50,1316,33],[1443,51,1316,34],[1444,2,1317,0],[1445,2,1318,0],[1446,0,1319,0],[1447,0,1320,0],[1448,0,1321,0],[1449,0,1322,0],[1450,0,1323,0],[1451,0,1324,0],[1452,0,1325,0],[1453,0,1326,0],[1454,0,1327,0],[1455,0,1328,0],[1456,0,1329,0],[1457,0,1330,0],[1458,0,1331,0],[1459,0,1332,0],[1460,0,1333,0],[1461,0,1334,0],[1462,0,1335,0],[1463,0,1336,0],[1464,0,1337,0],[1465,0,1338,0],[1466,0,1339,0],[1467,0,1340,0],[1468,0,1341,0],[1469,0,1342,0],[1470,0,1343,0],[1471,2,1344,0],[1472,2,1345,7],[1472,11,1345,16,"convLstm2d"],[1472,21,1345,26,"convLstm2d"],[1472,22,1345,27,"args"],[1472,26,1345,31],[1472,28,1345,33],[1473,4,1346,4],[1473,11,1346,11],[1473,15,1346,15,"ConvLSTM2D"],[1473,45,1346,25],[1473,46,1346,25,"ConvLSTM2D"],[1473,56,1346,25],[1473,57,1346,26,"args"],[1473,61,1346,30],[1473,62,1346,31],[1474,2,1347,0],[1475,2,1348,0],[1476,0,1349,0],[1477,0,1350,0],[1478,0,1351,0],[1479,0,1352,0],[1480,0,1353,0],[1481,0,1354,0],[1482,0,1355,0],[1483,0,1356,0],[1484,0,1357,0],[1485,0,1358,0],[1486,0,1359,0],[1487,0,1360,0],[1488,0,1361,0],[1489,0,1362,0],[1490,0,1363,0],[1491,0,1364,0],[1492,0,1365,0],[1493,0,1366,0],[1494,0,1367,0],[1495,0,1368,0],[1496,0,1369,0],[1497,0,1370,0],[1498,0,1371,0],[1499,0,1372,0],[1500,0,1373,0],[1501,0,1374,0],[1502,0,1375,0],[1503,0,1376,0],[1504,0,1377,0],[1505,0,1378,0],[1506,0,1379,0],[1507,2,1380,0],[1508,2,1381,7],[1508,11,1381,16,"convLstm2dCell"],[1508,25,1381,30,"convLstm2dCell"],[1508,26,1381,31,"args"],[1508,30,1381,35],[1508,32,1381,37],[1509,4,1382,4],[1509,11,1382,11],[1509,15,1382,15,"ConvLSTM2DCell"],[1509,45,1382,29],[1509,46,1382,29,"ConvLSTM2DCell"],[1509,60,1382,29],[1509,61,1382,30,"args"],[1509,65,1382,34],[1509,66,1382,35],[1510,2,1383,0],[1511,2,1384,0],[1512,0,1385,0],[1513,0,1386,0],[1514,0,1387,0],[1515,0,1388,0],[1516,0,1389,0],[1517,0,1390,0],[1518,0,1391,0],[1519,0,1392,0],[1520,0,1393,0],[1521,0,1394,0],[1522,0,1395,0],[1523,0,1396,0],[1524,0,1397,0],[1525,0,1398,0],[1526,0,1399,0],[1527,0,1400,0],[1528,0,1401,0],[1529,0,1402,0],[1530,0,1403,0],[1531,0,1404,0],[1532,0,1405,0],[1533,0,1406,0],[1534,0,1407,0],[1535,0,1408,0],[1536,0,1409,0],[1537,0,1410,0],[1538,0,1411,0],[1539,0,1412,0],[1540,0,1413,0],[1541,0,1414,0],[1542,0,1415,0],[1543,0,1416,0],[1544,0,1417,0],[1545,0,1418,0],[1546,0,1419,0],[1547,0,1420,0],[1548,0,1421,0],[1549,0,1422,0],[1550,0,1423,0],[1551,0,1424,0],[1552,0,1425,0],[1553,0,1426,0],[1554,0,1427,0],[1555,0,1428,0],[1556,0,1429,0],[1557,0,1430,0],[1558,0,1431,0],[1559,0,1432,0],[1560,0,1433,0],[1561,0,1434,0],[1562,0,1435,0],[1563,0,1436,0],[1564,0,1437,0],[1565,0,1438,0],[1566,0,1439,0],[1567,0,1440,0],[1568,0,1441,0],[1569,0,1442,0],[1570,0,1443,0],[1571,2,1444,7],[1571,11,1444,16,"rnn"],[1571,14,1444,19,"rnn"],[1571,15,1444,20,"args"],[1571,19,1444,24],[1571,21,1444,26],[1572,4,1445,4],[1572,11,1445,11],[1572,15,1445,15,"RNN"],[1572,31,1445,18],[1572,32,1445,18,"RNN"],[1572,35,1445,18],[1572,36,1445,19,"args"],[1572,40,1445,23],[1572,41,1445,24],[1573,2,1446,0],[1574,2,1447,0],[1575,0,1448,0],[1576,0,1449,0],[1577,0,1450,0],[1578,0,1451,0],[1579,0,1452,0],[1580,0,1453,0],[1581,2,1454,7],[1581,11,1454,16,"stackedRNNCells"],[1581,26,1454,31,"stackedRNNCells"],[1581,27,1454,32,"args"],[1581,31,1454,36],[1581,33,1454,38],[1582,4,1455,4],[1582,11,1455,11],[1582,15,1455,15,"StackedRNNCells"],[1582,31,1455,30],[1582,32,1455,30,"StackedRNNCells"],[1582,47,1455,30],[1582,48,1455,31,"args"],[1582,52,1455,35],[1582,53,1455,36],[1583,2,1456,0],[1584,2,1457,0],[1585,2,1458,0],[1586,2,1459,7],[1586,11,1459,16,"bidirectional"],[1586,24,1459,29,"bidirectional"],[1586,25,1459,30,"args"],[1586,29,1459,34],[1586,31,1459,36],[1587,4,1460,4],[1587,11,1460,11],[1587,15,1460,15,"Bidirectional"],[1587,30,1460,28],[1587,31,1460,28,"Bidirectional"],[1587,44,1460,28],[1587,45,1460,29,"args"],[1587,49,1460,33],[1587,50,1460,34],[1588,2,1461,0],[1589,2,1462,0],[1590,0,1463,0],[1591,0,1464,0],[1592,0,1465,0],[1593,0,1466,0],[1594,0,1467,0],[1595,0,1468,0],[1596,0,1469,0],[1597,0,1470,0],[1598,0,1471,0],[1599,0,1472,0],[1600,0,1473,0],[1601,0,1474,0],[1602,0,1475,0],[1603,0,1476,0],[1604,0,1477,0],[1605,0,1478,0],[1606,0,1479,0],[1607,0,1480,0],[1608,0,1481,0],[1609,0,1482,0],[1610,0,1483,0],[1611,0,1484,0],[1612,0,1485,0],[1613,0,1486,0],[1614,0,1487,0],[1615,0,1488,0],[1616,0,1489,0],[1617,0,1490,0],[1618,0,1491,0],[1619,0,1492,0],[1620,0,1493,0],[1621,0,1494,0],[1622,0,1495,0],[1623,0,1496,0],[1624,0,1497,0],[1625,0,1498,0],[1626,0,1499,0],[1627,0,1500,0],[1628,0,1501,0],[1629,0,1502,0],[1630,0,1503,0],[1631,0,1504,0],[1632,0,1505,0],[1633,0,1506,0],[1634,0,1507,0],[1635,2,1508,7],[1635,11,1508,16,"timeDistributed"],[1635,26,1508,31,"timeDistributed"],[1635,27,1508,32,"args"],[1635,31,1508,36],[1635,33,1508,38],[1636,4,1509,4],[1636,11,1509,11],[1636,15,1509,15,"TimeDistributed"],[1636,30,1509,30],[1636,31,1509,30,"TimeDistributed"],[1636,46,1509,30],[1636,47,1509,31,"args"],[1636,51,1509,35],[1636,52,1509,36],[1637,2,1510,0],[1638,2,1511,0],[1639,2,1512,7],[1639,8,1512,13,"globalMaxPool1d"],[1639,23,1512,28],[1639,26,1512,31,"globalMaxPooling1d"],[1639,44,1512,49],[1640,2,1513,7],[1640,8,1513,13,"globalMaxPool2d"],[1640,23,1513,28],[1640,26,1513,31,"globalMaxPooling2d"],[1640,44,1513,49],[1641,2,1514,7],[1641,8,1514,13,"maxPool1d"],[1641,17,1514,22],[1641,20,1514,25,"maxPooling1d"],[1641,32,1514,37],[1642,2,1515,7],[1642,8,1515,13,"maxPool2d"],[1642,17,1515,22],[1642,20,1515,25,"maxPooling2d"],[1642,32,1515,37],[1643,2,1517,0],[1644,0,1518,0],[1645,0,1519,0],[1646,0,1520,0],[1647,0,1521,0],[1648,0,1522,0],[1649,0,1523,0],[1650,0,1524,0],[1651,0,1525,0],[1652,0,1526,0],[1653,0,1527,0],[1654,0,1528,0],[1655,0,1529,0],[1656,0,1530,0],[1657,0,1531,0],[1658,0,1532,0],[1659,0,1533,0],[1660,0,1534,0],[1661,0,1535,0],[1662,0,1536,0],[1663,0,1537,0],[1664,0,1538,0],[1665,0,1539,0],[1666,2,1540,7],[1666,11,1540,16,"gaussianNoise"],[1666,24,1540,29,"gaussianNoise"],[1666,25,1540,30,"args"],[1666,29,1540,34],[1666,31,1540,36],[1667,4,1541,4],[1667,11,1541,11],[1667,15,1541,15,"GaussianNoise"],[1667,27,1541,28],[1667,28,1541,28,"GaussianNoise"],[1667,41,1541,28],[1667,42,1541,29,"args"],[1667,46,1541,33],[1667,47,1541,34],[1668,2,1542,0],[1669,2,1543,0],[1670,0,1544,0],[1671,0,1545,0],[1672,0,1546,0],[1673,0,1547,0],[1674,0,1548,0],[1675,0,1549,0],[1676,0,1550,0],[1677,0,1551,0],[1678,0,1552,0],[1679,0,1553,0],[1680,0,1554,0],[1681,0,1555,0],[1682,0,1556,0],[1683,0,1557,0],[1684,0,1558,0],[1685,0,1559,0],[1686,0,1560,0],[1687,0,1561,0],[1688,0,1562,0],[1689,0,1563,0],[1690,0,1564,0],[1691,0,1565,0],[1692,0,1566,0],[1693,2,1567,7],[1693,11,1567,16,"gaussianDropout"],[1693,26,1567,31,"gaussianDropout"],[1693,27,1567,32,"args"],[1693,31,1567,36],[1693,33,1567,38],[1694,4,1568,4],[1694,11,1568,11],[1694,15,1568,15,"GaussianDropout"],[1694,27,1568,30],[1694,28,1568,30,"GaussianDropout"],[1694,43,1568,30],[1694,44,1568,31,"args"],[1694,48,1568,35],[1694,49,1568,36],[1695,2,1569,0],[1696,2,1570,0],[1697,0,1571,0],[1698,0,1572,0],[1699,0,1573,0],[1700,0,1574,0],[1701,0,1575,0],[1702,0,1576,0],[1703,0,1577,0],[1704,0,1578,0],[1705,0,1579,0],[1706,0,1580,0],[1707,0,1581,0],[1708,0,1582,0],[1709,0,1583,0],[1710,0,1584,0],[1711,0,1585,0],[1712,0,1586,0],[1713,0,1587,0],[1714,0,1588,0],[1715,0,1589,0],[1716,0,1590,0],[1717,0,1591,0],[1718,0,1592,0],[1719,0,1593,0],[1720,0,1594,0],[1721,0,1595,0],[1722,0,1596,0],[1723,0,1597,0],[1724,0,1598,0],[1725,0,1599,0],[1726,0,1600,0],[1727,2,1601,7],[1727,11,1601,16,"alphaDropout"],[1727,23,1601,28,"alphaDropout"],[1727,24,1601,29,"args"],[1727,28,1601,33],[1727,30,1601,35],[1728,4,1602,4],[1728,11,1602,11],[1728,15,1602,15,"AlphaDropout"],[1728,27,1602,27],[1728,28,1602,27,"AlphaDropout"],[1728,40,1602,27],[1728,41,1602,28,"args"],[1728,45,1602,32],[1728,46,1602,33],[1729,2,1603,0],[1730,2,1604,0],[1731,0,1605,0],[1732,0,1606,0],[1733,0,1607,0],[1734,0,1608,0],[1735,0,1609,0],[1736,0,1610,0],[1737,0,1611,0],[1738,0,1612,0],[1739,0,1613,0],[1740,0,1614,0],[1741,0,1615,0],[1742,0,1616,0],[1743,0,1617,0],[1744,0,1618,0],[1745,0,1619,0],[1746,0,1620,0],[1747,0,1621,0],[1748,0,1622,0],[1749,0,1623,0],[1750,0,1624,0],[1751,0,1625,0],[1752,0,1626,0],[1753,2,1627,7],[1753,11,1627,16,"masking"],[1753,18,1627,23,"masking"],[1753,19,1627,24,"args"],[1753,23,1627,28],[1753,25,1627,30],[1754,4,1628,4],[1754,11,1628,11],[1754,15,1628,15,"Masking"],[1754,26,1628,22],[1754,27,1628,22,"Masking"],[1754,34,1628,22],[1754,35,1628,23,"args"],[1754,39,1628,27],[1754,40,1628,28],[1755,2,1629,0],[1756,2,1630,0],[1757,0,1631,0],[1758,0,1632,0],[1759,0,1633,0],[1760,0,1634,0],[1761,0,1635,0],[1762,0,1636,0],[1763,0,1637,0],[1764,0,1638,0],[1765,0,1639,0],[1766,0,1640,0],[1767,0,1641,0],[1768,0,1642,0],[1769,0,1643,0],[1770,0,1644,0],[1771,0,1645,0],[1772,0,1646,0],[1773,0,1647,0],[1774,0,1648,0],[1775,0,1649,0],[1776,0,1650,0],[1777,0,1651,0],[1778,0,1652,0],[1779,0,1653,0],[1780,0,1654,0],[1781,0,1655,0],[1782,0,1656,0],[1783,2,1657,7],[1783,11,1657,16,"rescaling"],[1783,20,1657,25,"rescaling"],[1783,21,1657,26,"args"],[1783,25,1657,30],[1783,27,1657,32],[1784,4,1658,4],[1784,11,1658,11],[1784,15,1658,15,"Rescaling"],[1784,54,1658,24],[1784,55,1658,24,"Rescaling"],[1784,64,1658,24],[1784,65,1658,25,"args"],[1784,69,1658,29],[1784,70,1658,30],[1785,2,1659,0],[1786,2,1660,0],[1787,0,1661,0],[1788,0,1662,0],[1789,0,1663,0],[1790,0,1664,0],[1791,0,1665,0],[1792,0,1666,0],[1793,0,1667,0],[1794,0,1668,0],[1795,0,1669,0],[1796,0,1670,0],[1797,0,1671,0],[1798,0,1672,0],[1799,0,1673,0],[1800,0,1674,0],[1801,0,1675,0],[1802,0,1676,0],[1803,0,1677,0],[1804,0,1678,0],[1805,0,1679,0],[1806,0,1680,0],[1807,0,1681,0],[1808,0,1682,0],[1809,0,1683,0],[1810,0,1684,0],[1811,0,1685,0],[1812,0,1686,0],[1813,0,1687,0],[1814,0,1688,0],[1815,2,1689,7],[1815,11,1689,16,"centerCrop"],[1815,21,1689,26,"centerCrop"],[1815,22,1689,27,"args"],[1815,26,1689,31],[1815,28,1689,33],[1816,4,1690,4],[1816,11,1690,11],[1816,15,1690,15,"CenterCrop"],[1816,46,1690,25],[1816,47,1690,25,"CenterCrop"],[1816,57,1690,25],[1816,58,1690,26,"args"],[1816,62,1690,30],[1816,63,1690,31],[1817,2,1691,0],[1818,2,1692,0],[1819,0,1693,0],[1820,0,1694,0],[1821,0,1695,0],[1822,0,1696,0],[1823,0,1697,0],[1824,0,1698,0],[1825,0,1699,0],[1826,0,1700,0],[1827,0,1701,0],[1828,0,1702,0],[1829,0,1703,0],[1830,0,1704,0],[1831,0,1705,0],[1832,0,1706,0],[1833,0,1707,0],[1834,0,1708,0],[1835,0,1709,0],[1836,0,1710,0],[1837,0,1711,0],[1838,0,1712,0],[1839,0,1713,0],[1840,2,1714,7],[1840,11,1714,16,"resizing"],[1840,19,1714,24,"resizing"],[1840,20,1714,25,"args"],[1840,24,1714,29],[1840,26,1714,31],[1841,4,1715,4],[1841,11,1715,11],[1841,15,1715,15,"Resizing"],[1841,49,1715,23],[1841,50,1715,23,"Resizing"],[1841,58,1715,23],[1841,59,1715,24,"args"],[1841,63,1715,28],[1841,64,1715,29],[1842,2,1716,0],[1843,2,1717,0],[1844,0,1718,0],[1845,0,1719,0],[1846,0,1720,0],[1847,0,1721,0],[1848,0,1722,0],[1849,0,1723,0],[1850,0,1724,0],[1851,0,1725,0],[1852,0,1726,0],[1853,0,1727,0],[1854,0,1728,0],[1855,0,1729,0],[1856,0,1730,0],[1857,0,1731,0],[1858,0,1732,0],[1859,0,1733,0],[1860,0,1734,0],[1861,0,1735,0],[1862,0,1736,0],[1863,0,1737,0],[1864,0,1738,0],[1865,0,1739,0],[1866,0,1740,0],[1867,0,1741,0],[1868,0,1742,0],[1869,0,1743,0],[1870,0,1744,0],[1871,0,1745,0],[1872,0,1746,0],[1873,0,1747,0],[1874,0,1748,0],[1875,0,1749,0],[1876,0,1750,0],[1877,0,1751,0],[1878,0,1752,0],[1879,0,1753,0],[1880,0,1754,0],[1881,0,1755,0],[1882,0,1756,0],[1883,0,1757,0],[1884,0,1758,0],[1885,0,1759,0],[1886,2,1760,7],[1886,11,1760,16,"categoryEncoding"],[1886,27,1760,32,"categoryEncoding"],[1886,28,1760,33,"args"],[1886,32,1760,37],[1886,34,1760,39],[1887,4,1761,4],[1887,11,1761,11],[1887,15,1761,15,"CategoryEncoding"],[1887,52,1761,31],[1887,53,1761,31,"CategoryEncoding"],[1887,69,1761,31],[1887,70,1761,32,"args"],[1887,74,1761,36],[1887,75,1761,37],[1888,2,1762,0],[1889,2,1763,0],[1890,0,1764,0],[1891,0,1765,0],[1892,0,1766,0],[1893,0,1767,0],[1894,0,1768,0],[1895,0,1769,0],[1896,0,1770,0],[1897,0,1771,0],[1898,0,1772,0],[1899,0,1773,0],[1900,0,1774,0],[1901,0,1775,0],[1902,0,1776,0],[1903,0,1777,0],[1904,0,1778,0],[1905,0,1779,0],[1906,0,1780,0],[1907,0,1781,0],[1908,0,1782,0],[1909,0,1783,0],[1910,0,1784,0],[1911,0,1785,0],[1912,0,1786,0],[1913,0,1787,0],[1914,0,1788,0],[1915,0,1789,0],[1916,0,1790,0],[1917,0,1791,0],[1918,0,1792,0],[1919,0,1793,0],[1920,0,1794,0],[1921,0,1795,0],[1922,0,1796,0],[1923,0,1797,0],[1924,0,1798,0],[1925,0,1799,0],[1926,0,1800,0],[1927,0,1801,0],[1928,0,1802,0],[1929,0,1803,0],[1930,0,1804,0],[1931,0,1805,0],[1932,0,1806,0],[1933,2,1807,7],[1933,11,1807,16,"randomWidth"],[1933,22,1807,27,"randomWidth"],[1933,23,1807,28,"args"],[1933,27,1807,32],[1933,29,1807,34],[1934,4,1808,4],[1934,11,1808,11],[1934,15,1808,15,"RandomWidth"],[1934,47,1808,26],[1934,48,1808,26,"RandomWidth"],[1934,59,1808,26],[1934,60,1808,27,"args"],[1934,64,1808,31],[1934,65,1808,32],[1935,2,1809,0],[1936,0,1809,1],[1936,3]],"functionMap":{"names":["<global>","inputLayer","elu","reLU","leakyReLU","prelu","softmax","thresholdedReLU","conv1d","conv2d","conv2dTranspose","conv3d","conv3dTranspose","separableConv2d","cropping2D","upSampling2d","depthwiseConv2d","activation","dense","dropout","spatialDropout1d","flatten","repeatVector","reshape","permute","embedding","add","average","concatenate","maximum","minimum","multiply","dot","batchNormalization","layerNormalization","zeroPadding2d","averagePooling1d","avgPool1d","avgPooling1d","averagePooling2d","avgPool2d","avgPooling2d","averagePooling3d","avgPool3d","avgPooling3d","globalAveragePooling1d","globalAveragePooling2d","globalMaxPooling1d","globalMaxPooling2d","maxPooling1d","maxPooling2d","maxPooling3d","gru","gruCell","lstm","lstmCell","simpleRNN","simpleRNNCell","convLstm2d","convLstm2dCell","rnn","stackedRNNCells","bidirectional","timeDistributed","gaussianNoise","gaussianDropout","alphaDropout","masking","rescaling","centerCrop","resizing","categoryEncoding","randomWidth"],"mappings":"AAA;OCoE;CDE;OE0B;CFE;OGkB;CHE;OIqB;CJE;OKsB;CLE;OMiB;CNE;OOyB;CPE;OQsB;CRE;OSmB;CTE;OUoC;CVE;OWmB;CXE;OYC;CZE;Oa8B;CbE;OcgC;CdE;OeyB;CfE;OgBY;ChBE;OiBiC;CjBE;OkBkC;ClBE;OmBW;CnBE;OoBiC;CpBE;OqBoB;CrBE;OsBe;CtBE;OuBwB;CvBE;OwB6B;CxBE;OyBY;CzBE;O0BsB;C1BE;O2BmB;C3BE;O4BqB;C5BE;O6BmB;C7BE;O8BmB;C9BE;O+BoB;C/BE;OgCuB;ChCE;OiCuB;CjCE;OkCqB;ClCE;OmCwB;CnCE;OoCa;CpCE;OqCC;CrCE;OsCG;CtCE;OuCwB;CvCE;OwCC;CxCE;OyCG;CzCE;O0CsB;C1CE;O2CC;C3CE;O4CG;C5CE;O6CU;C7CE;O8Ce;C9CE;O+CU;C/CE;OgDe;ChDE;OiDU;CjDE;OkDsB;ClDE;OmDsB;CnDE;OoDyB;CpDE;OqD8C;CrDE;OsDwB;CtDE;OuD8C;CvDE;OwDyB;CxDE;OyD8C;CzDE;O0D4B;C1DE;O2DkC;C3DE;O4D6D;C5DE;O6DQ;C7DE;O8DG;C9DE;O+D+C;C/DE;OgE8B;ChEE;OiEyB;CjEE;OkEgC;ClEE;OmEwB;CnEE;OoE4B;CpEE;OqE8B;CrEE;OsEuB;CtEE;OuE4C;CvEE;OwE6C;CxEE"},"hasCjsExports":false},"type":"js/module"}]}