{"dependencies":[{"name":"./adadelta_optimizer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":17,"column":0,"index":703},"end":{"line":17,"column":57,"index":760}}],"key":"7nNWKHjS0wE2PYx5+jqAFpJl6mI=","exportNames":["*"],"imports":1}},{"name":"./adagrad_optimizer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":18,"column":0,"index":761},"end":{"line":18,"column":55,"index":816}}],"key":"TCPs4ZEPwPGnnpT0xtXw7h3A7b8=","exportNames":["*"],"imports":1}},{"name":"./adam_optimizer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":19,"column":0,"index":817},"end":{"line":19,"column":49,"index":866}}],"key":"NvMKrjzYG2I/JEdfHFs9z8AwbXU=","exportNames":["*"],"imports":1}},{"name":"./adamax_optimizer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":20,"column":0,"index":867},"end":{"line":20,"column":53,"index":920}}],"key":"sQpl+R0m3BCe2gCGmZrh688XZZM=","exportNames":["*"],"imports":1}},{"name":"./momentum_optimizer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":21,"column":0,"index":921},"end":{"line":21,"column":57,"index":978}}],"key":"hz9VoiVcmWUBZSt4pQXCcjT0rX8=","exportNames":["*"],"imports":1}},{"name":"./rmsprop_optimizer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":22,"column":0,"index":979},"end":{"line":22,"column":55,"index":1034}}],"key":"twsdSKYif46O9N5QZKqhnaSXJ8E=","exportNames":["*"],"imports":1}},{"name":"./sgd_optimizer","data":{"asyncType":null,"isESMImport":true,"locs":[{"start":{"line":23,"column":0,"index":1035},"end":{"line":23,"column":47,"index":1082}}],"key":"m6LIkViFRHQ4ITsMzh9FQBK6xQ8=","exportNames":["*"],"imports":1}}],"output":[{"data":{"code":"__d(function (global, require, _$$_IMPORT_DEFAULT, _$$_IMPORT_ALL, module, exports, _dependencyMap) {\n  \"use strict\";\n\n  Object.defineProperty(exports, '__esModule', {\n    value: true\n  });\n  Object.defineProperty(exports, \"OptimizerConstructors\", {\n    enumerable: true,\n    get: function () {\n      return OptimizerConstructors;\n    }\n  });\n  var _adadelta_optimizer = require(_dependencyMap[0], \"./adadelta_optimizer\");\n  var _adagrad_optimizer = require(_dependencyMap[1], \"./adagrad_optimizer\");\n  var _adam_optimizer = require(_dependencyMap[2], \"./adam_optimizer\");\n  var _adamax_optimizer = require(_dependencyMap[3], \"./adamax_optimizer\");\n  var _momentum_optimizer = require(_dependencyMap[4], \"./momentum_optimizer\");\n  var _rmsprop_optimizer = require(_dependencyMap[5], \"./rmsprop_optimizer\");\n  var _sgd_optimizer = require(_dependencyMap[6], \"./sgd_optimizer\");\n  /**\n   * @license\n   * Copyright 2018 Google LLC. All Rights Reserved.\n   * Licensed under the Apache License, Version 2.0 (the \"License\");\n   * you may not use this file except in compliance with the License.\n   * You may obtain a copy of the License at\n   *\n   * http://www.apache.org/licenses/LICENSE-2.0\n   *\n   * Unless required by applicable law or agreed to in writing, software\n   * distributed under the License is distributed on an \"AS IS\" BASIS,\n   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   * See the License for the specific language governing permissions and\n   * limitations under the License.\n   * =============================================================================\n   */\n\n  class OptimizerConstructors {\n    /**\n     * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n     *\n     * ```js\n     * // Fit a quadratic function by learning the coefficients a, b, c.\n     * const xs = tf.tensor1d([0, 1, 2, 3]);\n     * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n     *\n     * const a = tf.scalar(Math.random()).variable();\n     * const b = tf.scalar(Math.random()).variable();\n     * const c = tf.scalar(Math.random()).variable();\n     *\n     * // y = a * x^2 + b * x + c.\n     * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n     * const loss = (pred, label) => pred.sub(label).square().mean();\n     *\n     * const learningRate = 0.01;\n     * const optimizer = tf.train.sgd(learningRate);\n     *\n     * // Train the model.\n     * for (let i = 0; i < 10; i++) {\n     *   optimizer.minimize(() => loss(f(xs), ys));\n     * }\n     *\n     * // Make predictions.\n     * console.log(\n     *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n     * const preds = f(xs).dataSync();\n     * preds.forEach((pred, i) => {\n     *   console.log(`x: ${i}, pred: ${pred}`);\n     * });\n     * ```\n     *\n     * @param learningRate The learning rate to use for the SGD algorithm.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static sgd(learningRate) {\n      return new _sgd_optimizer.SGDOptimizer(learningRate);\n    }\n    /**\n     * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n     * descent.\n     *\n     * See\n     * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n     * http://proceedings.mlr.press/v28/sutskever13.pdf)\n     *\n     * @param learningRate The learning rate to use for the Momentum gradient\n     * descent algorithm.\n     * @param momentum The momentum to use for the momentum gradient descent\n     * algorithm.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static momentum(learningRate, momentum, useNesterov = false) {\n      return new _momentum_optimizer.MomentumOptimizer(learningRate, momentum, useNesterov);\n    }\n    /**\n     * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n     * descent. This implementation uses plain momentum and is not centered\n     * version of RMSProp.\n     *\n     * See\n     * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n     * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     *\n     * @param learningRate The learning rate to use for the RMSProp gradient\n     * descent algorithm.\n     * @param decay The discounting factor for the history/coming gradient.\n     * @param momentum The momentum to use for the RMSProp gradient descent\n     * algorithm.\n     * @param epsilon Small value to avoid zero denominator.\n     * @param centered If true, gradients are normalized by the estimated\n     * variance of the gradient.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static rmsprop(learningRate, decay = .9, momentum = 0.0, epsilon = null, centered = false) {\n      return new _rmsprop_optimizer.RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    }\n    /**\n     * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n     *\n     * @param learningRate The learning rate to use for the Adam gradient\n     * descent algorithm.\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\n     * @param epsilon A small constant for numerical stability.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = null) {\n      return new _adam_optimizer.AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    }\n    /**\n     * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n     * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n     *\n     * @param learningRate The learning rate to use for the Adadelta gradient\n     * descent algorithm.\n     * @param rho The learning rate decay over each update.\n     * @param epsilon A constant epsilon used to better condition the grad\n     * update.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adadelta(learningRate = .001, rho = .95, epsilon = null) {\n      return new _adadelta_optimizer.AdadeltaOptimizer(learningRate, rho, epsilon);\n    }\n    /**\n     * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n     *\n     * @param learningRate The learning rate to use for the Adamax gradient\n     * descent algorithm.\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\n     * @param epsilon A small constant for numerical stability.\n     * @param decay The learning rate decay over each update.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = null, decay = 0.0) {\n      return new _adamax_optimizer.AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    }\n    /**\n     * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n     * See\n     * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n     * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     * or\n     * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n     * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n     *\n     * @param learningRate The learning rate to use for the Adagrad gradient\n     * descent algorithm.\n     * @param initialAccumulatorValue Starting value for the accumulators, must be\n     * positive.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adagrad(learningRate, initialAccumulatorValue = 0.1) {\n      return new _adagrad_optimizer.AdagradOptimizer(learningRate, initialAccumulatorValue);\n    }\n  }\n});","lineCount":185,"map":[[7,2,24,0,"Object"],[7,8,24,0],[7,9,24,0,"defineProperty"],[7,23,24,0],[7,24,24,0,"exports"],[7,31,24,0],[8,4,24,0,"enumerable"],[8,14,24,0],[9,4,24,0,"get"],[9,7,24,0],[9,18,24,0,"get"],[9,19,24,0],[10,6,24,0],[10,13,24,0,"OptimizerConstructors"],[10,34,24,0],[11,4,24,0],[12,2,24,0],[13,2,17,0],[13,6,17,0,"_adadelta_optimizer"],[13,25,17,0],[13,28,17,0,"require"],[13,35,17,0],[13,36,17,0,"_dependencyMap"],[13,50,17,0],[14,2,18,0],[14,6,18,0,"_adagrad_optimizer"],[14,24,18,0],[14,27,18,0,"require"],[14,34,18,0],[14,35,18,0,"_dependencyMap"],[14,49,18,0],[15,2,19,0],[15,6,19,0,"_adam_optimizer"],[15,21,19,0],[15,24,19,0,"require"],[15,31,19,0],[15,32,19,0,"_dependencyMap"],[15,46,19,0],[16,2,20,0],[16,6,20,0,"_adamax_optimizer"],[16,23,20,0],[16,26,20,0,"require"],[16,33,20,0],[16,34,20,0,"_dependencyMap"],[16,48,20,0],[17,2,21,0],[17,6,21,0,"_momentum_optimizer"],[17,25,21,0],[17,28,21,0,"require"],[17,35,21,0],[17,36,21,0,"_dependencyMap"],[17,50,21,0],[18,2,22,0],[18,6,22,0,"_rmsprop_optimizer"],[18,24,22,0],[18,27,22,0,"require"],[18,34,22,0],[18,35,22,0,"_dependencyMap"],[18,49,22,0],[19,2,23,0],[19,6,23,0,"_sgd_optimizer"],[19,20,23,0],[19,23,23,0,"require"],[19,30,23,0],[19,31,23,0,"_dependencyMap"],[19,45,23,0],[20,2,1,0],[21,0,2,0],[22,0,3,0],[23,0,4,0],[24,0,5,0],[25,0,6,0],[26,0,7,0],[27,0,8,0],[28,0,9,0],[29,0,10,0],[30,0,11,0],[31,0,12,0],[32,0,13,0],[33,0,14,0],[34,0,15,0],[35,0,16,0],[37,2,24,7],[37,8,24,13,"OptimizerConstructors"],[37,29,24,34],[37,30,24,35],[38,4,25,4],[39,0,26,0],[40,0,27,0],[41,0,28,0],[42,0,29,0],[43,0,30,0],[44,0,31,0],[45,0,32,0],[46,0,33,0],[47,0,34,0],[48,0,35,0],[49,0,36,0],[50,0,37,0],[51,0,38,0],[52,0,39,0],[53,0,40,0],[54,0,41,0],[55,0,42,0],[56,0,43,0],[57,0,44,0],[58,0,45,0],[59,0,46,0],[60,0,47,0],[61,0,48,0],[62,0,49,0],[63,0,50,0],[64,0,51,0],[65,0,52,0],[66,0,53,0],[67,0,54,0],[68,0,55,0],[69,0,56,0],[70,0,57,0],[71,0,58,0],[72,0,59,0],[73,0,60,0],[74,0,61,0],[75,4,62,4],[75,11,62,11,"sgd"],[75,14,62,14,"sgd"],[75,15,62,15,"learningRate"],[75,27,62,27],[75,29,62,29],[76,6,63,8],[76,13,63,15],[76,17,63,19,"SGDOptimizer"],[76,31,63,31],[76,32,63,31,"SGDOptimizer"],[76,44,63,31],[76,45,63,32,"learningRate"],[76,57,63,44],[76,58,63,45],[77,4,64,4],[78,4,65,4],[79,0,66,0],[80,0,67,0],[81,0,68,0],[82,0,69,0],[83,0,70,0],[84,0,71,0],[85,0,72,0],[86,0,73,0],[87,0,74,0],[88,0,75,0],[89,0,76,0],[90,0,77,0],[91,0,78,0],[92,0,79,0],[93,4,80,4],[93,11,80,11,"momentum"],[93,19,80,19,"momentum"],[93,20,80,20,"learningRate"],[93,32,80,32],[93,34,80,34,"momentum"],[93,42,80,42],[93,44,80,44,"useNesterov"],[93,55,80,55],[93,58,80,58],[93,63,80,63],[93,65,80,65],[94,6,81,8],[94,13,81,15],[94,17,81,19,"MomentumOptimizer"],[94,36,81,36],[94,37,81,36,"MomentumOptimizer"],[94,54,81,36],[94,55,81,37,"learningRate"],[94,67,81,49],[94,69,81,51,"momentum"],[94,77,81,59],[94,79,81,61,"useNesterov"],[94,90,81,72],[94,91,81,73],[95,4,82,4],[96,4,83,4],[97,0,84,0],[98,0,85,0],[99,0,86,0],[100,0,87,0],[101,0,88,0],[102,0,89,0],[103,0,90,0],[104,0,91,0],[105,0,92,0],[106,0,93,0],[107,0,94,0],[108,0,95,0],[109,0,96,0],[110,0,97,0],[111,0,98,0],[112,0,99,0],[113,0,100,0],[114,0,101,0],[115,0,102,0],[116,4,103,4],[116,11,103,11,"rmsprop"],[116,18,103,18,"rmsprop"],[116,19,103,19,"learningRate"],[116,31,103,31],[116,33,103,33,"decay"],[116,38,103,38],[116,41,103,41],[116,43,103,43],[116,45,103,45,"momentum"],[116,53,103,53],[116,56,103,56],[116,59,103,59],[116,61,103,61,"epsilon"],[116,68,103,68],[116,71,103,71],[116,75,103,75],[116,77,103,77,"centered"],[116,85,103,85],[116,88,103,88],[116,93,103,93],[116,95,103,95],[117,6,104,8],[117,13,104,15],[117,17,104,19,"RMSPropOptimizer"],[117,35,104,35],[117,36,104,35,"RMSPropOptimizer"],[117,52,104,35],[117,53,104,36,"learningRate"],[117,65,104,48],[117,67,104,50,"decay"],[117,72,104,55],[117,74,104,57,"momentum"],[117,82,104,65],[117,84,104,67,"epsilon"],[117,91,104,74],[117,93,104,76,"centered"],[117,101,104,84],[117,102,104,85],[118,4,105,4],[119,4,106,4],[120,0,107,0],[121,0,108,0],[122,0,109,0],[123,0,110,0],[124,0,111,0],[125,0,112,0],[126,0,113,0],[127,0,114,0],[128,0,115,0],[129,0,116,0],[130,0,117,0],[131,4,118,4],[131,11,118,11,"adam"],[131,15,118,15,"adam"],[131,16,118,16,"learningRate"],[131,28,118,28],[131,31,118,31],[131,36,118,36],[131,38,118,38,"beta1"],[131,43,118,43],[131,46,118,46],[131,49,118,49],[131,51,118,51,"beta2"],[131,56,118,56],[131,59,118,59],[131,64,118,64],[131,66,118,66,"epsilon"],[131,73,118,73],[131,76,118,76],[131,80,118,80],[131,82,118,82],[132,6,119,8],[132,13,119,15],[132,17,119,19,"AdamOptimizer"],[132,32,119,32],[132,33,119,32,"AdamOptimizer"],[132,46,119,32],[132,47,119,33,"learningRate"],[132,59,119,45],[132,61,119,47,"beta1"],[132,66,119,52],[132,68,119,54,"beta2"],[132,73,119,59],[132,75,119,61,"epsilon"],[132,82,119,68],[132,83,119,69],[133,4,120,4],[134,4,121,4],[135,0,122,0],[136,0,123,0],[137,0,124,0],[138,0,125,0],[139,0,126,0],[140,0,127,0],[141,0,128,0],[142,0,129,0],[143,0,130,0],[144,0,131,0],[145,0,132,0],[146,4,133,4],[146,11,133,11,"adadelta"],[146,19,133,19,"adadelta"],[146,20,133,20,"learningRate"],[146,32,133,32],[146,35,133,35],[146,39,133,39],[146,41,133,41,"rho"],[146,44,133,44],[146,47,133,47],[146,50,133,50],[146,52,133,52,"epsilon"],[146,59,133,59],[146,62,133,62],[146,66,133,66],[146,68,133,68],[147,6,134,8],[147,13,134,15],[147,17,134,19,"AdadeltaOptimizer"],[147,36,134,36],[147,37,134,36,"AdadeltaOptimizer"],[147,54,134,36],[147,55,134,37,"learningRate"],[147,67,134,49],[147,69,134,51,"rho"],[147,72,134,54],[147,74,134,56,"epsilon"],[147,81,134,63],[147,82,134,64],[148,4,135,4],[149,4,136,4],[150,0,137,0],[151,0,138,0],[152,0,139,0],[153,0,140,0],[154,0,141,0],[155,0,142,0],[156,0,143,0],[157,0,144,0],[158,0,145,0],[159,0,146,0],[160,0,147,0],[161,0,148,0],[162,4,149,4],[162,11,149,11,"adamax"],[162,17,149,17,"adamax"],[162,18,149,18,"learningRate"],[162,30,149,30],[162,33,149,33],[162,38,149,38],[162,40,149,40,"beta1"],[162,45,149,45],[162,48,149,48],[162,51,149,51],[162,53,149,53,"beta2"],[162,58,149,58],[162,61,149,61],[162,66,149,66],[162,68,149,68,"epsilon"],[162,75,149,75],[162,78,149,78],[162,82,149,82],[162,84,149,84,"decay"],[162,89,149,89],[162,92,149,92],[162,95,149,95],[162,97,149,97],[163,6,150,8],[163,13,150,15],[163,17,150,19,"AdamaxOptimizer"],[163,34,150,34],[163,35,150,34,"AdamaxOptimizer"],[163,50,150,34],[163,51,150,35,"learningRate"],[163,63,150,47],[163,65,150,49,"beta1"],[163,70,150,54],[163,72,150,56,"beta2"],[163,77,150,61],[163,79,150,63,"epsilon"],[163,86,150,70],[163,88,150,72,"decay"],[163,93,150,77],[163,94,150,78],[164,4,151,4],[165,4,152,4],[166,0,153,0],[167,0,154,0],[168,0,155,0],[169,0,156,0],[170,0,157,0],[171,0,158,0],[172,0,159,0],[173,0,160,0],[174,0,161,0],[175,0,162,0],[176,0,163,0],[177,0,164,0],[178,0,165,0],[179,0,166,0],[180,0,167,0],[181,4,168,4],[181,11,168,11,"adagrad"],[181,18,168,18,"adagrad"],[181,19,168,19,"learningRate"],[181,31,168,31],[181,33,168,33,"initialAccumulatorValue"],[181,56,168,56],[181,59,168,59],[181,62,168,62],[181,64,168,64],[182,6,169,8],[182,13,169,15],[182,17,169,19,"AdagradOptimizer"],[182,35,169,35],[182,36,169,35,"AdagradOptimizer"],[182,52,169,35],[182,53,169,36,"learningRate"],[182,65,169,48],[182,67,169,50,"initialAccumulatorValue"],[182,90,169,73],[182,91,169,74],[183,4,170,4],[184,2,171,0],[185,0,171,1],[185,3]],"functionMap":{"names":["<global>","OptimizerConstructors","OptimizerConstructors.sgd","OptimizerConstructors.momentum","OptimizerConstructors.rmsprop","OptimizerConstructors.adam","OptimizerConstructors.adadelta","OptimizerConstructors.adamax","OptimizerConstructors.adagrad"],"mappings":"AAA;OCuB;ICsC;KDE;IEgB;KFE;IGqB;KHE;IIa;KJE;IKa;KLE;IMc;KNE;IOiB;KPE;CDC"},"hasCjsExports":false},"type":"js/module"}]}